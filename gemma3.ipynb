{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1b280ab-b61f-4d1a-bf7e-44e5f9ed3a5c",
   "metadata": {
    "id": "e1b280ab-b61f-4d1a-bf7e-44e5f9ed3a5c"
   },
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efde77f2-6af3-4781-8597-89ecd3f41a52",
   "metadata": {
    "id": "efde77f2-6af3-4781-8597-89ecd3f41a52"
   },
   "source": [
    "# Gemma 3 270M From Scratch (A Standalone Notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cdef4d-de59-4a65-89f9-fa2a8ef3471d",
   "metadata": {
    "id": "55cdef4d-de59-4a65-89f9-fa2a8ef3471d"
   },
   "source": [
    "- This notebook is purposefully minimal and focuses on the code to re-implement Gemma 3 270M in pure PyTorch without relying on other external LLM libraries\n",
    "- For more information, see the official [Gemma 3 270M model card](https://huggingface.co/google/gemma-3-270m)\n",
    "\n",
    "- Below is a side-by-side comparison with Qwen3 0.6B as a reference model; if you are interested in the Qwen3 0.6B standalone notebook, you can find it [here](../11_qwen3)\n",
    "<br>\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/gemma3/gemma3-vs-qwen3.webp?1\">\n",
    "  \n",
    "  \n",
    "- About the code:\n",
    "  - all code is my own code, mapping the Gemma 3 architecture onto the model code implemented in my [Build A Large Language Model (From Scratch)](http://mng.bz/orYv) book; the code is released under a permissive open-source Apache 2.0 license (see [LICENSE.txt](https://github.com/rasbt/LLMs-from-scratch/blob/main/LICENSE.txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c201adb-747e-437b-9a62-442802941e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd1b65a8-4301-444a-bd7c-a6f2bd1df9df",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dd1b65a8-4301-444a-bd7c-a6f2bd1df9df",
    "outputId": "4f762354-e0a3-4cc2-e5d4-e61a227a202c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface_hub version: 0.30.2\n",
      "tokenizers version: 0.21.1\n",
      "torch version: 2.7.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\n",
    "    \"huggingface_hub\",  # to download pretrained weights\n",
    "    \"tokenizers\",       # to implement the tokenizer\n",
    "    \"torch\",            # to implement the model\n",
    "]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e96fbb-8e16-4f6d-835f-c6159321280b",
   "metadata": {},
   "source": [
    "- This notebook supports both the base model and the instructmodel; which model to use can be controlled via the following flag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70a90338-624a-4706-aa55-6b4358070194",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_INSTRUCT_MODEL = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653410a6-dd2b-4eb2-a722-23d9782e726d",
   "metadata": {
    "id": "653410a6-dd2b-4eb2-a722-23d9782e726d"
   },
   "source": [
    "&nbsp;\n",
    "# 1. Architecture code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82076c21-9331-4dcd-b017-42b046cf1a60",
   "metadata": {
    "id": "82076c21-9331-4dcd-b017-42b046cf1a60"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nix/store/v1bkq59yxcf4ys3np121vpmb2abgnh72-python3.12-torch-2.7.0/lib/python3.12/site-packages/torch/_subclasses/functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /build/pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        self.fc2 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        self.fc3 = nn.Linear(cfg[\"hidden_dim\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_fc1 = self.fc1(x)\n",
    "        x_fc2 = self.fc2(x)\n",
    "        x = nn.functional.gelu(x_fc1, approximate=\"tanh\") * x_fc2\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56715760-37e1-433e-89da-04864c139a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, emb_dim, eps=1e-6, bias=False):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        # Gemma3 stores zero-centered weights and uses (1 + weight) during forward\n",
    "        self.scale = nn.Parameter(torch.zeros(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim)) if bias else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Match HF Gemma3: compute norm in float32, then scale by (1 + w)\n",
    "        input_dtype = x.dtype\n",
    "        x_f = x.float()\n",
    "        var = x_f.pow(2).mean(dim=-1, keepdim=True)\n",
    "        x_norm = x_f * torch.rsqrt(var + self.eps)\n",
    "        out = x_norm * (1.0 + self.scale.float())\n",
    "\n",
    "        if self.shift is not None:\n",
    "            out = out + self.shift.float()\n",
    "\n",
    "        return out.to(input_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b9a346f-5826-4083-9162-abd56afc03f0",
   "metadata": {
    "id": "4b9a346f-5826-4083-9162-abd56afc03f0"
   },
   "outputs": [],
   "source": [
    "def compute_rope_params(head_dim, theta_base=10_000, context_length=4096, dtype=torch.float32):\n",
    "    assert head_dim % 2 == 0, \"Embedding dimension must be even\"\n",
    "\n",
    "    # Compute the inverse frequencies\n",
    "    inv_freq = 1.0 / (theta_base ** (torch.arange(0, head_dim, 2, dtype=dtype)[: (head_dim // 2)].float() / head_dim))\n",
    "\n",
    "    # Generate position indices\n",
    "    positions = torch.arange(context_length, dtype=dtype)\n",
    "\n",
    "    # Compute the angles\n",
    "    angles = positions[:, None] * inv_freq[None, :]  # Shape: (context_length, head_dim // 2)\n",
    "\n",
    "    # Expand angles to match the head_dim\n",
    "    angles = torch.cat([angles, angles], dim=1)  # Shape: (context_length, head_dim)\n",
    "\n",
    "    # Precompute sine and cosine\n",
    "    cos = torch.cos(angles)\n",
    "    sin = torch.sin(angles)\n",
    "\n",
    "    return cos, sin\n",
    "\n",
    "\n",
    "def apply_rope(x, cos, sin):\n",
    "    # x: (batch_size, num_heads, seq_len, head_dim)\n",
    "    batch_size, num_heads, seq_len, head_dim = x.shape\n",
    "    assert head_dim % 2 == 0, \"Head dimension must be even\"\n",
    "\n",
    "    # Split x into first half and second half\n",
    "    x1 = x[..., : head_dim // 2]  # First half\n",
    "    x2 = x[..., head_dim // 2 :]  # Second half\n",
    "\n",
    "    # Adjust sin and cos shapes\n",
    "    cos = cos[:seq_len, :].unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, seq_len, head_dim)\n",
    "    sin = sin[:seq_len, :].unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # Apply the rotary transformation\n",
    "    rotated = torch.cat((-x2, x1), dim=-1)\n",
    "    x_rotated = (x * cos) + (rotated * sin)\n",
    "\n",
    "    # It's ok to use lower-precision after applying cos and sin rotation\n",
    "    return x_rotated.to(dtype=x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8169ab5-f976-4222-a2e1-eb1cabf267cb",
   "metadata": {
    "id": "e8169ab5-f976-4222-a2e1-eb1cabf267cb"
   },
   "outputs": [],
   "source": [
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self, d_in, num_heads, num_kv_groups, head_dim=None, qk_norm=False,\n",
    "        query_pre_attn_scalar=None, dtype=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert num_heads % num_kv_groups == 0, \"num_heads must be divisible by num_kv_groups\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.num_kv_groups = num_kv_groups\n",
    "        self.group_size = num_heads // num_kv_groups\n",
    "\n",
    "        if head_dim is None:\n",
    "            assert d_in % num_heads == 0, \"`d_in` must be divisible by `num_heads` if `head_dim` is not set\"\n",
    "            head_dim = d_in // num_heads\n",
    "\n",
    "        self.head_dim = head_dim\n",
    "        self.d_out = num_heads * head_dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, self.d_out, bias=False, dtype=dtype)\n",
    "        self.W_key = nn.Linear(d_in, num_kv_groups * head_dim, bias=False, dtype=dtype)\n",
    "        self.W_value = nn.Linear(d_in, num_kv_groups * head_dim, bias=False, dtype=dtype)\n",
    "\n",
    "        self.out_proj = nn.Linear(self.d_out, d_in, bias=False, dtype=dtype)\n",
    "\n",
    "        if qk_norm:\n",
    "            self.q_norm = RMSNorm(head_dim, eps=1e-6)\n",
    "            self.k_norm = RMSNorm(head_dim, eps=1e-6)\n",
    "        else:\n",
    "            self.q_norm = self.k_norm = None\n",
    "\n",
    "        if query_pre_attn_scalar is not None:\n",
    "            self.scaling = (query_pre_attn_scalar) ** -0.5\n",
    "        else:\n",
    "            self.scaling = (head_dim) ** -0.5\n",
    "\n",
    "\n",
    "    def forward(self, x, mask, cos, sin):\n",
    "        b, num_tokens, _ = x.shape\n",
    "\n",
    "        # Apply projections\n",
    "        queries = self.W_query(x)  # (b, num_tokens, num_heads * head_dim)(1,3,1024)\n",
    "        keys = self.W_key(x)       # (b, num_tokens, num_kv_groups * head_dim)\n",
    "        values = self.W_value(x)   # (b, num_tokens, num_kv_groups * head_dim)\n",
    "\n",
    "        print(queries)\n",
    "        print(\"initial querys shape:\", queries.shape)\n",
    "\n",
    "        # Reshape\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        print(queries)\n",
    "        print(\"after view querys shape:\", queries.shape)\n",
    "\n",
    "        queries = queries.transpose(1, 2)  # (b, num_heads, num_tokens, head_dim)\n",
    "        print(queries)\n",
    "        print(\"after transpose querys shape:\", queries.shape)\n",
    "\n",
    "        keys = keys.view(b, num_tokens, self.num_kv_groups, self.head_dim).transpose(1, 2)\n",
    "        values = values.view(b, num_tokens, self.num_kv_groups, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Optional normalization\n",
    "        if self.q_norm:\n",
    "            queries = self.q_norm(queries)\n",
    "        if self.k_norm:\n",
    "            keys = self.k_norm(keys)\n",
    "\n",
    "        # Apply RoPE\n",
    "        queries = apply_rope(queries, cos, sin)\n",
    "        keys = apply_rope(keys, cos, sin)\n",
    "\n",
    "        # Expand K and V to match number of heads\n",
    "        keys = keys.repeat_interleave(self.group_size, dim=1)\n",
    "        values = values.repeat_interleave(self.group_size, dim=1)\n",
    "\n",
    "        # Scale queries\n",
    "        queries = queries * self.scaling\n",
    "\n",
    "        # Attention\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        attn_scores = attn_scores.masked_fill(mask, -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        context = (attn_weights @ values).transpose(1, 2).reshape(b, num_tokens, self.d_out)\n",
    "        return self.out_proj(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "457cb2f8-50c1-4045-8a74-f181bfb5fea9",
   "metadata": {
    "id": "457cb2f8-50c1-4045-8a74-f181bfb5fea9"
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg: dict, attn_type: str):\n",
    "        super().__init__()\n",
    "        self.attn_type = attn_type\n",
    "\n",
    "        self.att = GroupedQueryAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            num_kv_groups=cfg[\"n_kv_groups\"],\n",
    "            head_dim=cfg[\"head_dim\"],\n",
    "            qk_norm=cfg[\"qk_norm\"],\n",
    "            query_pre_attn_scalar=cfg[\"query_pre_attn_scalar\"],\n",
    "            dtype=cfg[\"dtype\"],\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.input_layernorm = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
    "        self.post_attention_layernorm = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
    "        self.pre_feedforward_layernorm = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
    "        self.post_feedforward_layernorm = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        mask_global,\n",
    "        mask_local,\n",
    "        cos_global,\n",
    "        sin_global,\n",
    "        cos_local,\n",
    "        sin_local,\n",
    "    ):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.input_layernorm(x)\n",
    "\n",
    "        if self.attn_type == \"sliding_attention\":\n",
    "            attn_mask = mask_local\n",
    "            cos = cos_local\n",
    "            sin = sin_local\n",
    "        else:\n",
    "            attn_mask = mask_global\n",
    "            cos = cos_global\n",
    "            sin = sin_global\n",
    "\n",
    "        x_attn = self.att(x, attn_mask, cos, sin)\n",
    "        x_attn = self.post_attention_layernorm(x_attn)\n",
    "        x = shortcut + x_attn\n",
    "\n",
    "        # Shortcut connection for feed forward block\n",
    "        shortcut = x\n",
    "        x_ffn = self.pre_feedforward_layernorm(x)\n",
    "        x_ffn = self.ff(x_ffn)\n",
    "        x_ffn = self.post_feedforward_layernorm(x_ffn)\n",
    "        x = shortcut + x_ffn\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e88de3e3-9f07-42cc-816b-28dbd46e96c4",
   "metadata": {
    "id": "e88de3e3-9f07-42cc-816b-28dbd46e96c4"
   },
   "outputs": [],
   "source": [
    "class Gemma3Model(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        assert cfg[\"layer_types\"] is not None and len(cfg[\"layer_types\"]) == cfg[\"n_layers\"]\n",
    "\n",
    "        # Main model parameters\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"])\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(cfg, attn_type)for attn_type in cfg[\"layer_types\"]\n",
    "        ])\n",
    "\n",
    "        self.final_norm = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False, dtype=cfg[\"dtype\"])\n",
    "        self.cfg = cfg\n",
    "\n",
    "        # Reusuable utilities\n",
    "        cos_local, sin_local = compute_rope_params(\n",
    "            head_dim=cfg[\"head_dim\"],\n",
    "            theta_base=cfg[\"rope_local_base\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            dtype=torch.float32,\n",
    "        )\n",
    "        cos_global, sin_global = compute_rope_params(\n",
    "            head_dim=cfg[\"head_dim\"],\n",
    "            theta_base=cfg[\"rope_base\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            dtype=torch.float32,\n",
    "        )\n",
    "        self.register_buffer(\"cos_local\", cos_local, persistent=False)\n",
    "        self.register_buffer(\"sin_local\", sin_local, persistent=False)\n",
    "        self.register_buffer(\"cos_global\", cos_global, persistent=False)\n",
    "        self.register_buffer(\"sin_global\", sin_global, persistent=False)\n",
    "\n",
    "    def _create_masks(self, seq_len, device):\n",
    "        ones = torch.ones((seq_len, seq_len), dtype=torch.bool, device=device)\n",
    "\n",
    "        # mask_global (future is masked: j > i)\n",
    "        #     j:  0 1 2 3 4 5 6 7\n",
    "        #  i\n",
    "        #     0:  0 1 1 1 1 1 1 1\n",
    "        #     1:  0 0 1 1 1 1 1 1\n",
    "        #     2:  0 0 0 1 1 1 1 1\n",
    "        #     3:  0 0 0 0 1 1 1 1\n",
    "        #     4:  0 0 0 0 0 1 1 1\n",
    "        #     5:  0 0 0 0 0 0 1 1\n",
    "        #     6:  0 0 0 0 0 0 0 1\n",
    "        #     7:  0 0 0 0 0 0 0 0\n",
    "        mask_global = torch.triu(ones, diagonal=1)\n",
    "\n",
    "        # far_past (too far back is masked: i - j >= sliding_window)\n",
    "        # where sliding_window = 4\n",
    "        #     j:  0 1 2 3 4 5 6 7\n",
    "        #  i\n",
    "        #     0:  0 0 0 0 0 0 0 0\n",
    "        #     1:  0 0 0 0 0 0 0 0\n",
    "        #     2:  0 0 0 0 0 0 0 0\n",
    "        #     3:  0 0 0 0 0 0 0 0\n",
    "        #     4:  1 0 0 0 0 0 0 0\n",
    "        #     5:  1 1 0 0 0 0 0 0\n",
    "        #     6:  1 1 1 0 0 0 0 0\n",
    "        #     7:  1 1 1 1 0 0 0 0\n",
    "        far_past = torch.triu(ones, diagonal=self.cfg[\"sliding_window\"]).T\n",
    "\n",
    "        # Local (sliding_window) = future OR far-past\n",
    "        # mask_local\n",
    "        #     j:  0 1 2 3 4 5 6 7\n",
    "        # i\n",
    "        # 0:      0 1 1 1 1 1 1 1\n",
    "        # 1:      0 0 1 1 1 1 1 1\n",
    "        # 2:      0 0 0 1 1 1 1 1\n",
    "        # 3:      0 0 0 0 1 1 1 1\n",
    "        # 4:      1 0 0 0 0 1 1 1\n",
    "        # 5:      1 1 0 0 0 0 1 1\n",
    "        # 6:      1 1 1 0 0 0 0 1\n",
    "        # 7:      1 1 1 1 0 0 0 0\n",
    "        mask_local = mask_global | far_past\n",
    "        return mask_global, mask_local\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # Forward pass\n",
    "        b, seq_len = input_ids.shape\n",
    "        x = self.tok_emb(input_ids) * (self.cfg[\"emb_dim\"] ** 0.5)\n",
    "        mask_global, mask_local = self._create_masks(seq_len, x.device)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(\n",
    "                x,\n",
    "                mask_global=mask_global,\n",
    "                mask_local=mask_local,\n",
    "                cos_global=self.cos_global,\n",
    "                sin_global=self.sin_global,\n",
    "                cos_local=self.cos_local,\n",
    "                sin_local=self.sin_local,\n",
    "            )\n",
    "\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x.to(self.cfg[\"dtype\"]))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2d201f-74ad-4d63-ab9c-601b00674a48",
   "metadata": {
    "id": "be2d201f-74ad-4d63-ab9c-601b00674a48"
   },
   "source": [
    "&nbsp;\n",
    "# 2. Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "caa142fa-b375-4e78-b392-2072ced666f3",
   "metadata": {
    "id": "caa142fa-b375-4e78-b392-2072ced666f3"
   },
   "outputs": [],
   "source": [
    "GEMMA3_CONFIG_270M = {\n",
    "    \"vocab_size\": 262_144,\n",
    "    \"context_length\": 32_768,\n",
    "    \"emb_dim\": 640,\n",
    "    \"n_heads\": 4,\n",
    "    \"n_layers\": 18,\n",
    "    \"hidden_dim\": 2048,\n",
    "    \"head_dim\": 256,\n",
    "    \"qk_norm\": True,\n",
    "    \"n_kv_groups\": 1,\n",
    "    \"rope_local_base\": 10_000.0,\n",
    "    \"rope_base\": 1_000_000.0,\n",
    "    \"sliding_window\": 512,\n",
    "      \"layer_types\": [\n",
    "        \"sliding_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"full_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"full_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"full_attention\"\n",
    "    ],\n",
    "    \"dtype\": torch.bfloat16,\n",
    "    \"query_pre_attn_scalar\": 256,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "156253fe-aacd-4da2-8f13-705f05c4b11e",
   "metadata": {
    "id": "156253fe-aacd-4da2-8f13-705f05c4b11e"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "model = Gemma3Model(GEMMA3_CONFIG_270M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eaf86265-4e9d-4024-9ed0-99076944e304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma3Model(\n",
       "  (tok_emb): Embedding(262144, 640)\n",
       "  (blocks): ModuleList(\n",
       "    (0-17): 18 x TransformerBlock(\n",
       "      (att): GroupedQueryAttention(\n",
       "        (W_query): Linear(in_features=640, out_features=1024, bias=False)\n",
       "        (W_key): Linear(in_features=640, out_features=256, bias=False)\n",
       "        (W_value): Linear(in_features=640, out_features=256, bias=False)\n",
       "        (out_proj): Linear(in_features=1024, out_features=640, bias=False)\n",
       "        (q_norm): RMSNorm()\n",
       "        (k_norm): RMSNorm()\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=640, out_features=2048, bias=False)\n",
       "        (fc2): Linear(in_features=640, out_features=2048, bias=False)\n",
       "        (fc3): Linear(in_features=2048, out_features=640, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "      (pre_feedforward_layernorm): RMSNorm()\n",
       "      (post_feedforward_layernorm): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (final_norm): RMSNorm()\n",
       "  (out_head): Linear(in_features=640, out_features=262144, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aca91d-4bee-45ce-993a-4ec5393abe2b",
   "metadata": {},
   "source": [
    "- A quick check that the forward pass works before continuing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "adf0a6b7-b688-42c9-966e-c223d34db99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4824,  1.3672,  0.5391,  ..., -0.3066, -1.6484,  0.2266],\n",
      "         [-0.5156,  0.6406, -0.5938,  ..., -0.5352,  0.1709, -0.4902],\n",
      "         [ 0.1465, -0.9453, -0.3438,  ..., -0.1006,  0.0175, -0.3574]]],\n",
      "       dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)\n",
      "initial querys shape: torch.Size([1, 3, 1024])\n",
      "tensor([[[[-0.4824,  1.3672,  0.5391,  ..., -0.2617, -0.5039, -0.2021],\n",
      "          [ 0.0265,  0.6445, -0.3535,  ...,  0.2637,  0.5469,  0.8633],\n",
      "          [ 0.3516, -0.3301,  0.4902,  ..., -0.0086, -1.0703, -0.2852],\n",
      "          [ 1.0625, -0.5000, -0.0859,  ..., -0.3066, -1.6484,  0.2266]],\n",
      "\n",
      "         [[-0.5156,  0.6406, -0.5938,  ...,  0.1602, -0.6328, -0.1025],\n",
      "          [-1.0078,  1.2109, -0.2129,  ...,  0.6523, -0.1504,  0.3945],\n",
      "          [-0.4141,  0.4121, -0.5742,  ...,  0.5625, -0.9258, -0.2461],\n",
      "          [-0.6367, -0.3594, -0.4922,  ..., -0.5352,  0.1709, -0.4902]],\n",
      "\n",
      "         [[ 0.1465, -0.9453, -0.3438,  ..., -0.3594,  0.1572, -0.6797],\n",
      "          [ 0.3105, -0.3730, -0.5312,  ...,  0.7070, -1.4688,  0.3418],\n",
      "          [ 0.3711, -0.1904, -0.3359,  ...,  0.4434,  0.6406, -1.2266],\n",
      "          [ 0.0659, -0.5117,  0.1924,  ..., -0.1006,  0.0175, -0.3574]]]],\n",
      "       dtype=torch.bfloat16, grad_fn=<ViewBackward0>)\n",
      "after view querys shape: torch.Size([1, 3, 4, 256])\n",
      "tensor([[[[-0.4824,  1.3672,  0.5391,  ..., -0.2617, -0.5039, -0.2021],\n",
      "          [-0.5156,  0.6406, -0.5938,  ...,  0.1602, -0.6328, -0.1025],\n",
      "          [ 0.1465, -0.9453, -0.3438,  ..., -0.3594,  0.1572, -0.6797]],\n",
      "\n",
      "         [[ 0.0265,  0.6445, -0.3535,  ...,  0.2637,  0.5469,  0.8633],\n",
      "          [-1.0078,  1.2109, -0.2129,  ...,  0.6523, -0.1504,  0.3945],\n",
      "          [ 0.3105, -0.3730, -0.5312,  ...,  0.7070, -1.4688,  0.3418]],\n",
      "\n",
      "         [[ 0.3516, -0.3301,  0.4902,  ..., -0.0086, -1.0703, -0.2852],\n",
      "          [-0.4141,  0.4121, -0.5742,  ...,  0.5625, -0.9258, -0.2461],\n",
      "          [ 0.3711, -0.1904, -0.3359,  ...,  0.4434,  0.6406, -1.2266]],\n",
      "\n",
      "         [[ 1.0625, -0.5000, -0.0859,  ..., -0.3066, -1.6484,  0.2266],\n",
      "          [-0.6367, -0.3594, -0.4922,  ..., -0.5352,  0.1709, -0.4902],\n",
      "          [ 0.0659, -0.5117,  0.1924,  ..., -0.1006,  0.0175, -0.3574]]]],\n",
      "       dtype=torch.bfloat16, grad_fn=<TransposeBackward0>)\n",
      "after transpose querys shape: torch.Size([1, 4, 3, 256])\n",
      "tensor([[[-0.2227,  0.2871, -0.0713,  ..., -1.7578, -0.3633,  0.6914],\n",
      "         [-0.1416,  0.5625,  0.0199,  ..., -0.6328,  0.9062,  0.6641],\n",
      "         [-0.4336,  0.7734, -0.8516,  ..., -0.5938,  0.0219, -0.7227]]],\n",
      "       dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)\n",
      "initial querys shape: torch.Size([1, 3, 1024])\n",
      "tensor([[[[-0.2227,  0.2871, -0.0713,  ..., -0.5000, -0.2812,  0.3809],\n",
      "          [ 0.1953, -0.4473,  0.7852,  ..., -0.3672,  0.5664,  0.4336],\n",
      "          [ 0.1436,  0.4277,  0.7539,  ..., -0.6211, -0.1318,  0.1543],\n",
      "          [-0.3867, -0.4727,  0.1992,  ..., -1.7578, -0.3633,  0.6914]],\n",
      "\n",
      "         [[-0.1416,  0.5625,  0.0199,  ...,  1.0000, -0.9297,  0.9805],\n",
      "          [-0.2793, -0.0757, -0.9727,  ..., -0.0625,  0.0182, -0.3379],\n",
      "          [ 0.2793,  0.7539, -0.4180,  ...,  0.6680, -0.0488,  0.5312],\n",
      "          [ 1.2656, -1.1641, -0.2334,  ..., -0.6328,  0.9062,  0.6641]],\n",
      "\n",
      "         [[-0.4336,  0.7734, -0.8516,  ..., -0.6562, -1.3203,  0.3340],\n",
      "          [-0.1846, -0.6367,  0.1328,  ..., -0.8281, -0.6172,  0.4102],\n",
      "          [ 0.0625, -0.4082,  0.1172,  ...,  0.3379, -0.9570, -0.1436],\n",
      "          [-0.4082, -0.6445,  0.1748,  ..., -0.5938,  0.0219, -0.7227]]]],\n",
      "       dtype=torch.bfloat16, grad_fn=<ViewBackward0>)\n",
      "after view querys shape: torch.Size([1, 3, 4, 256])\n",
      "tensor([[[[-0.2227,  0.2871, -0.0713,  ..., -0.5000, -0.2812,  0.3809],\n",
      "          [-0.1416,  0.5625,  0.0199,  ...,  1.0000, -0.9297,  0.9805],\n",
      "          [-0.4336,  0.7734, -0.8516,  ..., -0.6562, -1.3203,  0.3340]],\n",
      "\n",
      "         [[ 0.1953, -0.4473,  0.7852,  ..., -0.3672,  0.5664,  0.4336],\n",
      "          [-0.2793, -0.0757, -0.9727,  ..., -0.0625,  0.0182, -0.3379],\n",
      "          [-0.1846, -0.6367,  0.1328,  ..., -0.8281, -0.6172,  0.4102]],\n",
      "\n",
      "         [[ 0.1436,  0.4277,  0.7539,  ..., -0.6211, -0.1318,  0.1543],\n",
      "          [ 0.2793,  0.7539, -0.4180,  ...,  0.6680, -0.0488,  0.5312],\n",
      "          [ 0.0625, -0.4082,  0.1172,  ...,  0.3379, -0.9570, -0.1436]],\n",
      "\n",
      "         [[-0.3867, -0.4727,  0.1992,  ..., -1.7578, -0.3633,  0.6914],\n",
      "          [ 1.2656, -1.1641, -0.2334,  ..., -0.6328,  0.9062,  0.6641],\n",
      "          [-0.4082, -0.6445,  0.1748,  ..., -0.5938,  0.0219, -0.7227]]]],\n",
      "       dtype=torch.bfloat16, grad_fn=<TransposeBackward0>)\n",
      "after transpose querys shape: torch.Size([1, 4, 3, 256])\n",
      "tensor([[[-0.0864,  0.3613, -1.1641,  ..., -0.9102,  1.1641,  0.3496],\n",
      "         [ 0.4395,  0.5234,  0.4551,  ..., -0.6836, -0.6211, -0.3340],\n",
      "         [-0.2637,  0.4922,  0.5586,  ..., -0.3516, -0.0618,  0.3066]]],\n",
      "       dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)\n",
      "initial querys shape: torch.Size([1, 3, 1024])\n",
      "tensor([[[[-0.0864,  0.3613, -1.1641,  ..., -0.2969, -0.1885, -0.9844],\n",
      "          [-0.6133,  0.1943,  0.6797,  ...,  1.3359, -0.5352, -0.6992],\n",
      "          [ 0.9570,  0.3945, -0.2070,  ...,  0.5234, -0.3008, -0.2832],\n",
      "          [ 0.9258,  0.0032,  0.7656,  ..., -0.9102,  1.1641,  0.3496]],\n",
      "\n",
      "         [[ 0.4395,  0.5234,  0.4551,  ...,  0.0488,  0.2139, -0.4316],\n",
      "          [ 1.6172, -0.5078,  0.2715,  ...,  0.0649,  0.3594,  0.1641],\n",
      "          [-0.0908, -0.2891,  0.0466,  ..., -0.7578,  0.2178,  0.5469],\n",
      "          [ 0.4238,  0.5273, -0.7148,  ..., -0.6836, -0.6211, -0.3340]],\n",
      "\n",
      "         [[-0.2637,  0.4922,  0.5586,  ...,  0.5234, -0.0140,  0.4609],\n",
      "          [ 0.1167,  0.1797,  0.2949,  ...,  0.3613,  0.1060,  0.0137],\n",
      "          [ 0.3770, -1.0234, -0.6250,  ...,  0.0752,  0.2363, -0.3047],\n",
      "          [-0.7227, -0.8945, -1.0391,  ..., -0.3516, -0.0618,  0.3066]]]],\n",
      "       dtype=torch.bfloat16, grad_fn=<ViewBackward0>)\n",
      "after view querys shape: torch.Size([1, 3, 4, 256])\n",
      "tensor([[[[-0.0864,  0.3613, -1.1641,  ..., -0.2969, -0.1885, -0.9844],\n",
      "          [ 0.4395,  0.5234,  0.4551,  ...,  0.0488,  0.2139, -0.4316],\n",
      "          [-0.2637,  0.4922,  0.5586,  ...,  0.5234, -0.0140,  0.4609]],\n",
      "\n",
      "         [[-0.6133,  0.1943,  0.6797,  ...,  1.3359, -0.5352, -0.6992],\n",
      "          [ 1.6172, -0.5078,  0.2715,  ...,  0.0649,  0.3594,  0.1641],\n",
      "          [ 0.1167,  0.1797,  0.2949,  ...,  0.3613,  0.1060,  0.0137]],\n",
      "\n",
      "         [[ 0.9570,  0.3945, -0.2070,  ...,  0.5234, -0.3008, -0.2832],\n",
      "          [-0.0908, -0.2891,  0.0466,  ..., -0.7578,  0.2178,  0.5469],\n",
      "          [ 0.3770, -1.0234, -0.6250,  ...,  0.0752,  0.2363, -0.3047]],\n",
      "\n",
      "         [[ 0.9258,  0.0032,  0.7656,  ..., -0.9102,  1.1641,  0.3496],\n",
      "          [ 0.4238,  0.5273, -0.7148,  ..., -0.6836, -0.6211, -0.3340],\n",
      "          [-0.7227, -0.8945, -1.0391,  ..., -0.3516, -0.0618,  0.3066]]]],\n",
      "       dtype=torch.bfloat16, grad_fn=<TransposeBackward0>)\n",
      "after transpose querys shape: torch.Size([1, 4, 3, 256])\n",
      "tensor([[[ 8.3594e-01, -1.1670e-01,  4.1211e-01,  ..., -1.2812e+00,\n",
      "          -9.5215e-02, -3.5352e-01],\n",
      "         [ 9.3750e-01, -5.0293e-02,  1.8555e-01,  ...,  5.6641e-01,\n",
      "          -8.9844e-01,  5.1172e-01],\n",
      "         [-1.6309e-01,  4.8637e-04,  5.2344e-01,  ..., -7.2656e-01,\n",
      "          -4.2578e-01, -1.0469e+00]]], dtype=torch.bfloat16,\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "initial querys shape: torch.Size([1, 3, 1024])\n",
      "tensor([[[[ 8.3594e-01, -1.1670e-01,  4.1211e-01,  ...,  7.0703e-01,\n",
      "           -2.6758e-01,  6.1328e-01],\n",
      "          [-4.6875e-02,  1.0645e-01,  1.2634e-02,  ..., -5.2734e-01,\n",
      "            4.4922e-02,  1.6016e-01],\n",
      "          [-4.9609e-01, -1.8164e-01,  2.5977e-01,  ..., -8.7500e-01,\n",
      "           -4.9414e-01, -1.4609e+00],\n",
      "          [ 9.0234e-01,  5.7422e-01, -7.2754e-02,  ..., -1.2812e+00,\n",
      "           -9.5215e-02, -3.5352e-01]],\n",
      "\n",
      "         [[ 9.3750e-01, -5.0293e-02,  1.8555e-01,  ...,  1.0303e-01,\n",
      "           -1.2207e-01,  1.1475e-01],\n",
      "          [-6.7578e-01,  4.6875e-01,  2.5586e-01,  ..., -5.7812e-01,\n",
      "           -3.7695e-01,  3.4375e-01],\n",
      "          [ 7.4219e-01,  1.0205e-01, -3.7354e-02,  ...,  2.8320e-01,\n",
      "           -6.8750e-01, -7.7734e-01],\n",
      "          [-2.4414e-01, -1.5234e-01,  1.3379e-01,  ...,  5.6641e-01,\n",
      "           -8.9844e-01,  5.1172e-01]],\n",
      "\n",
      "         [[-1.6309e-01,  4.8637e-04,  5.2344e-01,  ...,  2.4512e-01,\n",
      "           -7.6172e-02,  1.9727e-01],\n",
      "          [-1.0312e+00,  2.4023e-01, -5.2979e-02,  ..., -1.1597e-02,\n",
      "           -1.1406e+00,  2.2852e-01],\n",
      "          [-1.0303e-01,  1.8848e-01,  1.4453e-01,  ..., -9.8438e-01,\n",
      "           -3.6377e-02, -9.5703e-01],\n",
      "          [ 1.9043e-01, -5.0391e-01,  4.8438e-01,  ..., -7.2656e-01,\n",
      "           -4.2578e-01, -1.0469e+00]]]], dtype=torch.bfloat16,\n",
      "       grad_fn=<ViewBackward0>)\n",
      "after view querys shape: torch.Size([1, 3, 4, 256])\n",
      "tensor([[[[ 8.3594e-01, -1.1670e-01,  4.1211e-01,  ...,  7.0703e-01,\n",
      "           -2.6758e-01,  6.1328e-01],\n",
      "          [ 9.3750e-01, -5.0293e-02,  1.8555e-01,  ...,  1.0303e-01,\n",
      "           -1.2207e-01,  1.1475e-01],\n",
      "          [-1.6309e-01,  4.8637e-04,  5.2344e-01,  ...,  2.4512e-01,\n",
      "           -7.6172e-02,  1.9727e-01]],\n",
      "\n",
      "         [[-4.6875e-02,  1.0645e-01,  1.2634e-02,  ..., -5.2734e-01,\n",
      "            4.4922e-02,  1.6016e-01],\n",
      "          [-6.7578e-01,  4.6875e-01,  2.5586e-01,  ..., -5.7812e-01,\n",
      "           -3.7695e-01,  3.4375e-01],\n",
      "          [-1.0312e+00,  2.4023e-01, -5.2979e-02,  ..., -1.1597e-02,\n",
      "           -1.1406e+00,  2.2852e-01]],\n",
      "\n",
      "         [[-4.9609e-01, -1.8164e-01,  2.5977e-01,  ..., -8.7500e-01,\n",
      "           -4.9414e-01, -1.4609e+00],\n",
      "          [ 7.4219e-01,  1.0205e-01, -3.7354e-02,  ...,  2.8320e-01,\n",
      "           -6.8750e-01, -7.7734e-01],\n",
      "          [-1.0303e-01,  1.8848e-01,  1.4453e-01,  ..., -9.8438e-01,\n",
      "           -3.6377e-02, -9.5703e-01]],\n",
      "\n",
      "         [[ 9.0234e-01,  5.7422e-01, -7.2754e-02,  ..., -1.2812e+00,\n",
      "           -9.5215e-02, -3.5352e-01],\n",
      "          [-2.4414e-01, -1.5234e-01,  1.3379e-01,  ...,  5.6641e-01,\n",
      "           -8.9844e-01,  5.1172e-01],\n",
      "          [ 1.9043e-01, -5.0391e-01,  4.8438e-01,  ..., -7.2656e-01,\n",
      "           -4.2578e-01, -1.0469e+00]]]], dtype=torch.bfloat16,\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "after transpose querys shape: torch.Size([1, 4, 3, 256])\n",
      "tensor([[[ 0.0145, -0.3027,  0.0996,  ...,  0.7500, -0.1299, -1.1719],\n",
      "         [-0.4355,  0.2988,  0.4863,  ..., -0.1001,  0.4609,  0.0640],\n",
      "         [ 0.0162, -0.9727, -0.2969,  ...,  0.3867,  0.4297, -0.0337]]],\n",
      "       dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)\n",
      "initial querys shape: torch.Size([1, 3, 1024])\n",
      "tensor([[[[ 0.0145, -0.3027,  0.0996,  ...,  0.4609,  0.5312, -0.4512],\n",
      "          [ 0.3887, -0.0139, -0.0947,  ...,  0.1416,  0.5977,  1.0625],\n",
      "          [ 0.4199, -0.2695, -0.2354,  ...,  0.0055, -1.0547, -0.1953],\n",
      "          [-0.5625, -0.0674,  1.0391,  ...,  0.7500, -0.1299, -1.1719]],\n",
      "\n",
      "         [[-0.4355,  0.2988,  0.4863,  ...,  0.1338, -0.6016,  0.3184],\n",
      "          [-0.3203,  0.7070, -1.4219,  ...,  0.4316, -0.3398,  0.2520],\n",
      "          [ 0.3516,  0.5742, -1.2031,  ..., -0.5547, -0.0825,  1.0781],\n",
      "          [ 0.3398, -0.1484, -0.0630,  ..., -0.1001,  0.4609,  0.0640]],\n",
      "\n",
      "         [[ 0.0162, -0.9727, -0.2969,  ...,  0.2676,  0.1660,  0.2520],\n",
      "          [ 0.1602, -0.7539,  0.2715,  ..., -0.2676,  0.6406, -0.3750],\n",
      "          [-0.3613, -0.1396,  0.6289,  ...,  0.6719,  0.2656, -0.2471],\n",
      "          [-0.5703, -0.5391,  0.7656,  ...,  0.3867,  0.4297, -0.0337]]]],\n",
      "       dtype=torch.bfloat16, grad_fn=<ViewBackward0>)\n",
      "after view querys shape: torch.Size([1, 3, 4, 256])\n",
      "tensor([[[[ 0.0145, -0.3027,  0.0996,  ...,  0.4609,  0.5312, -0.4512],\n",
      "          [-0.4355,  0.2988,  0.4863,  ...,  0.1338, -0.6016,  0.3184],\n",
      "          [ 0.0162, -0.9727, -0.2969,  ...,  0.2676,  0.1660,  0.2520]],\n",
      "\n",
      "         [[ 0.3887, -0.0139, -0.0947,  ...,  0.1416,  0.5977,  1.0625],\n",
      "          [-0.3203,  0.7070, -1.4219,  ...,  0.4316, -0.3398,  0.2520],\n",
      "          [ 0.1602, -0.7539,  0.2715,  ..., -0.2676,  0.6406, -0.3750]],\n",
      "\n",
      "         [[ 0.4199, -0.2695, -0.2354,  ...,  0.0055, -1.0547, -0.1953],\n",
      "          [ 0.3516,  0.5742, -1.2031,  ..., -0.5547, -0.0825,  1.0781],\n",
      "          [-0.3613, -0.1396,  0.6289,  ...,  0.6719,  0.2656, -0.2471]],\n",
      "\n",
      "         [[-0.5625, -0.0674,  1.0391,  ...,  0.7500, -0.1299, -1.1719],\n",
      "          [ 0.3398, -0.1484, -0.0630,  ..., -0.1001,  0.4609,  0.0640],\n",
      "          [-0.5703, -0.5391,  0.7656,  ...,  0.3867,  0.4297, -0.0337]]]],\n",
      "       dtype=torch.bfloat16, grad_fn=<TransposeBackward0>)\n",
      "after transpose querys shape: torch.Size([1, 4, 3, 256])\n",
      "tensor([[[ 0.5156,  0.0256,  0.7461,  ..., -0.8711,  0.9023, -1.1250],\n",
      "         [ 0.0017,  0.4395, -0.1836,  ...,  1.0859, -0.1904,  0.0752],\n",
      "         [-1.2969, -0.9609, -0.0459,  ..., -0.3223, -0.8750,  0.4160]]],\n",
      "       dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)\n",
      "initial querys shape: torch.Size([1, 3, 1024])\n",
      "tensor([[[[ 0.5156,  0.0256,  0.7461,  ...,  0.1533,  0.3262,  1.5625],\n",
      "          [-0.5117, -0.4922, -0.2793,  ..., -0.5469,  0.2490,  1.3281],\n",
      "          [-0.2695,  0.8633,  0.0289,  ...,  1.0312, -0.4453, -0.2334],\n",
      "          [ 0.9844, -1.1016,  0.8672,  ..., -0.8711,  0.9023, -1.1250]],\n",
      "\n",
      "         [[ 0.0017,  0.4395, -0.1836,  ..., -1.4609,  0.4531, -0.3457],\n",
      "          [-0.0684, -0.7148, -0.0039,  ..., -0.2559,  0.3906,  0.2500],\n",
      "          [ 0.4121,  0.2754,  1.2031,  ...,  0.1279, -0.0364, -0.1030],\n",
      "          [-0.7461,  0.1943, -0.4570,  ...,  1.0859, -0.1904,  0.0752]],\n",
      "\n",
      "         [[-1.2969, -0.9609, -0.0459,  ..., -0.2598,  0.6523,  0.5352],\n",
      "          [ 0.8125,  0.1328,  0.3809,  ..., -0.6445,  0.0928,  0.4766],\n",
      "          [-0.2236, -1.1016, -0.4355,  ..., -0.2383,  1.1875,  1.4062],\n",
      "          [-0.8750,  0.2305,  0.0270,  ..., -0.3223, -0.8750,  0.4160]]]],\n",
      "       dtype=torch.bfloat16, grad_fn=<ViewBackward0>)\n",
      "after view querys shape: torch.Size([1, 3, 4, 256])\n",
      "tensor([[[[ 0.5156,  0.0256,  0.7461,  ...,  0.1533,  0.3262,  1.5625],\n",
      "          [ 0.0017,  0.4395, -0.1836,  ..., -1.4609,  0.4531, -0.3457],\n",
      "          [-1.2969, -0.9609, -0.0459,  ..., -0.2598,  0.6523,  0.5352]],\n",
      "\n",
      "         [[-0.5117, -0.4922, -0.2793,  ..., -0.5469,  0.2490,  1.3281],\n",
      "          [-0.0684, -0.7148, -0.0039,  ..., -0.2559,  0.3906,  0.2500],\n",
      "          [ 0.8125,  0.1328,  0.3809,  ..., -0.6445,  0.0928,  0.4766]],\n",
      "\n",
      "         [[-0.2695,  0.8633,  0.0289,  ...,  1.0312, -0.4453, -0.2334],\n",
      "          [ 0.4121,  0.2754,  1.2031,  ...,  0.1279, -0.0364, -0.1030],\n",
      "          [-0.2236, -1.1016, -0.4355,  ..., -0.2383,  1.1875,  1.4062]],\n",
      "\n",
      "         [[ 0.9844, -1.1016,  0.8672,  ..., -0.8711,  0.9023, -1.1250],\n",
      "          [-0.7461,  0.1943, -0.4570,  ...,  1.0859, -0.1904,  0.0752],\n",
      "          [-0.8750,  0.2305,  0.0270,  ..., -0.3223, -0.8750,  0.4160]]]],\n",
      "       dtype=torch.bfloat16, grad_fn=<TransposeBackward0>)\n",
      "after transpose querys shape: torch.Size([1, 4, 3, 256])\n",
      "tensor([[[-0.2695,  0.9375, -0.2090,  ...,  0.1670, -0.1816, -1.4531],\n",
      "         [ 0.8750, -0.1270,  0.0299,  ..., -0.1226,  0.1250,  0.2207],\n",
      "         [ 0.0046,  0.3594, -0.5078,  ..., -0.5352,  0.1494, -0.6445]]],\n",
      "       dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)\n",
      "initial querys shape: torch.Size([1, 3, 1024])\n",
      "tensor([[[[-0.2695,  0.9375, -0.2090,  ...,  0.4473,  0.0131, -0.3301],\n",
      "          [ 0.6133,  0.7422, -0.1108,  ..., -0.1895,  0.2656, -0.1152],\n",
      "          [-0.3320,  0.5156,  0.2988,  ...,  0.4688, -0.1396,  0.6836],\n",
      "          [-0.0243, -0.6719,  1.3203,  ...,  0.1670, -0.1816, -1.4531]],\n",
      "\n",
      "         [[ 0.8750, -0.1270,  0.0299,  ..., -1.3281,  0.3320,  1.4609],\n",
      "          [ 0.5273,  0.4102,  0.3418,  ..., -0.5352,  0.1875, -0.4453],\n",
      "          [-0.2256,  0.5664,  0.0615,  ..., -0.3164,  0.0200, -0.4922],\n",
      "          [ 0.5039,  0.7305, -0.5312,  ..., -0.1226,  0.1250,  0.2207]],\n",
      "\n",
      "         [[ 0.0046,  0.3594, -0.5078,  ...,  0.2295,  0.2334,  0.8555],\n",
      "          [ 0.8477, -0.0796, -0.6445,  ...,  0.5703,  0.4609,  0.0996],\n",
      "          [-0.3066, -0.4512, -0.2129,  ..., -0.1445, -0.4570, -0.9023],\n",
      "          [-0.1768, -0.2969, -0.0469,  ..., -0.5352,  0.1494, -0.6445]]]],\n",
      "       dtype=torch.bfloat16, grad_fn=<ViewBackward0>)\n",
      "after view querys shape: torch.Size([1, 3, 4, 256])\n",
      "tensor([[[[-0.2695,  0.9375, -0.2090,  ...,  0.4473,  0.0131, -0.3301],\n",
      "          [ 0.8750, -0.1270,  0.0299,  ..., -1.3281,  0.3320,  1.4609],\n",
      "          [ 0.0046,  0.3594, -0.5078,  ...,  0.2295,  0.2334,  0.8555]],\n",
      "\n",
      "         [[ 0.6133,  0.7422, -0.1108,  ..., -0.1895,  0.2656, -0.1152],\n",
      "          [ 0.5273,  0.4102,  0.3418,  ..., -0.5352,  0.1875, -0.4453],\n",
      "          [ 0.8477, -0.0796, -0.6445,  ...,  0.5703,  0.4609,  0.0996]],\n",
      "\n",
      "         [[-0.3320,  0.5156,  0.2988,  ...,  0.4688, -0.1396,  0.6836],\n",
      "          [-0.2256,  0.5664,  0.0615,  ..., -0.3164,  0.0200, -0.4922],\n",
      "          [-0.3066, -0.4512, -0.2129,  ..., -0.1445, -0.4570, -0.9023]],\n",
      "\n",
      "         [[-0.0243, -0.6719,  1.3203,  ...,  0.1670, -0.1816, -1.4531],\n",
      "          [ 0.5039,  0.7305, -0.5312,  ..., -0.1226,  0.1250,  0.2207],\n",
      "          [-0.1768, -0.2969, -0.0469,  ..., -0.5352,  0.1494, -0.6445]]]],\n",
      "       dtype=torch.bfloat16, grad_fn=<TransposeBackward0>)\n",
      "after transpose querys shape: torch.Size([1, 4, 3, 256])\n",
      "tensor([[[-0.0605,  0.3945,  1.4766,  ...,  0.9883,  0.5742, -0.6094],\n",
      "         [-0.2988,  1.2266,  0.4141,  ..., -0.9180,  0.6445,  0.3184],\n",
      "         [ 0.3281, -1.2344,  0.0128,  ..., -0.7188,  0.9297,  0.4492]]],\n",
      "       dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)\n",
      "initial querys shape: torch.Size([1, 3, 1024])\n",
      "tensor([[[[-0.0605,  0.3945,  1.4766,  ...,  0.9883, -1.0234,  0.0322],\n",
      "          [-0.3145,  0.0879,  0.4707,  ...,  0.1934,  1.0000, -0.8242],\n",
      "          [ 1.0859, -0.1816,  0.4336,  ..., -0.2207,  0.2793, -0.1367],\n",
      "          [-0.1680, -0.4707,  0.0258,  ...,  0.9883,  0.5742, -0.6094]],\n",
      "\n",
      "         [[-0.2988,  1.2266,  0.4141,  ...,  0.6445, -0.3438,  0.5977],\n",
      "          [-0.1875, -0.5781, -0.4512,  ...,  0.0742,  0.1836,  0.9102],\n",
      "          [ 0.2158,  0.3926,  0.4316,  ...,  1.2344, -0.4316, -1.4922],\n",
      "          [ 1.4062,  0.3203, -1.2891,  ..., -0.9180,  0.6445,  0.3184]],\n",
      "\n",
      "         [[ 0.3281, -1.2344,  0.0128,  ..., -0.1143, -0.2451,  0.8672],\n",
      "          [ 0.4082, -0.3164,  0.1357,  ..., -0.1147,  0.5586, -0.2314],\n",
      "          [ 0.1177,  0.2578,  0.9805,  ..., -0.1138, -0.3496,  0.1562],\n",
      "          [-0.5078, -0.5469, -1.1953,  ..., -0.7188,  0.9297,  0.4492]]]],\n",
      "       dtype=torch.bfloat16, grad_fn=<ViewBackward0>)\n",
      "after view querys shape: torch.Size([1, 3, 4, 256])\n",
      "tensor([[[[-0.0605,  0.3945,  1.4766,  ...,  0.9883, -1.0234,  0.0322],\n",
      "          [-0.2988,  1.2266,  0.4141,  ...,  0.6445, -0.3438,  0.5977],\n",
      "          [ 0.3281, -1.2344,  0.0128,  ..., -0.1143, -0.2451,  0.8672]],\n",
      "\n",
      "         [[-0.3145,  0.0879,  0.4707,  ...,  0.1934,  1.0000, -0.8242],\n",
      "          [-0.1875, -0.5781, -0.4512,  ...,  0.0742,  0.1836,  0.9102],\n",
      "          [ 0.4082, -0.3164,  0.1357,  ..., -0.1147,  0.5586, -0.2314]],\n",
      "\n",
      "         [[ 1.0859, -0.1816,  0.4336,  ..., -0.2207,  0.2793, -0.1367],\n",
      "          [ 0.2158,  0.3926,  0.4316,  ...,  1.2344, -0.4316, -1.4922],\n",
      "          [ 0.1177,  0.2578,  0.9805,  ..., -0.1138, -0.3496,  0.1562]],\n",
      "\n",
      "         [[-0.1680, -0.4707,  0.0258,  ...,  0.9883,  0.5742, -0.6094],\n",
      "          [ 1.4062,  0.3203, -1.2891,  ..., -0.9180,  0.6445,  0.3184],\n",
      "          [-0.5078, -0.5469, -1.1953,  ..., -0.7188,  0.9297,  0.4492]]]],\n",
      "       dtype=torch.bfloat16, grad_fn=<TransposeBackward0>)\n",
      "after transpose querys shape: torch.Size([1, 4, 3, 256])\n",
      "tensor([[[-0.4570, -0.0287, -0.0125,  ...,  0.7773, -0.3164, -0.8945],\n",
      "         [ 0.1270,  0.2217,  0.6055,  ..., -0.4121,  0.0459, -0.5039],\n",
      "         [ 0.6016, -0.0073, -1.2344,  ...,  0.0236, -0.1270,  0.3516]]],\n",
      "       dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)\n",
      "initial querys shape: torch.Size([1, 3, 1024])\n",
      "tensor([[[[-0.4570, -0.0287, -0.0125,  ..., -0.0962, -0.1924, -0.2539],\n",
      "          [ 0.0278,  0.5586, -0.0317,  ..., -0.8555, -0.2061,  0.1436],\n",
      "          [ 0.7422, -0.2695, -0.2598,  ...,  0.3281,  1.1641, -0.1235],\n",
      "          [ 0.0410,  0.1699, -0.5117,  ...,  0.7773, -0.3164, -0.8945]],\n",
      "\n",
      "         [[ 0.1270,  0.2217,  0.6055,  ..., -0.0024, -0.4980,  0.2832],\n",
      "          [ 1.3203,  0.4551, -0.4609,  ...,  0.0298, -0.2754,  0.4062],\n",
      "          [-0.9883,  0.9648,  0.4219,  ...,  0.2246,  0.0520,  0.4863],\n",
      "          [ 0.7812,  1.2656, -0.7227,  ..., -0.4121,  0.0459, -0.5039]],\n",
      "\n",
      "         [[ 0.6016, -0.0073, -1.2344,  ..., -0.1670, -0.2471,  0.8164],\n",
      "          [-0.9414, -0.1436, -0.7656,  ..., -0.2432,  0.1660, -0.8555],\n",
      "          [ 0.9023,  0.3398,  0.8945,  ..., -0.1875,  0.4902,  0.5000],\n",
      "          [-1.1328,  0.3574,  0.4629,  ...,  0.0236, -0.1270,  0.3516]]]],\n",
      "       dtype=torch.bfloat16, grad_fn=<ViewBackward0>)\n",
      "after view querys shape: torch.Size([1, 3, 4, 256])\n",
      "tensor([[[[-0.4570, -0.0287, -0.0125,  ..., -0.0962, -0.1924, -0.2539],\n",
      "          [ 0.1270,  0.2217,  0.6055,  ..., -0.0024, -0.4980,  0.2832],\n",
      "          [ 0.6016, -0.0073, -1.2344,  ..., -0.1670, -0.2471,  0.8164]],\n",
      "\n",
      "         [[ 0.0278,  0.5586, -0.0317,  ..., -0.8555, -0.2061,  0.1436],\n",
      "          [ 1.3203,  0.4551, -0.4609,  ...,  0.0298, -0.2754,  0.4062],\n",
      "          [-0.9414, -0.1436, -0.7656,  ..., -0.2432,  0.1660, -0.8555]],\n",
      "\n",
      "         [[ 0.7422, -0.2695, -0.2598,  ...,  0.3281,  1.1641, -0.1235],\n",
      "          [-0.9883,  0.9648,  0.4219,  ...,  0.2246,  0.0520,  0.4863],\n",
      "          [ 0.9023,  0.3398,  0.8945,  ..., -0.1875,  0.4902,  0.5000]],\n",
      "\n",
      "         [[ 0.0410,  0.1699, -0.5117,  ...,  0.7773, -0.3164, -0.8945],\n",
      "          [ 0.7812,  1.2656, -0.7227,  ..., -0.4121,  0.0459, -0.5039],\n",
      "          [-1.1328,  0.3574,  0.4629,  ...,  0.0236, -0.1270,  0.3516]]]],\n",
      "       dtype=torch.bfloat16, grad_fn=<TransposeBackward0>)\n",
      "after transpose querys shape: torch.Size([1, 4, 3, 256])\n",
      "tensor([[[-0.6250,  0.0942,  1.2578,  ..., -0.6953,  0.4863, -0.1602],\n",
      "         [-0.0052,  0.0801, -0.5039,  ..., -1.1875,  0.0981, -0.7344],\n",
      "         [-0.1113,  0.2891, -0.6367,  ...,  0.2383, -0.8008, -0.3125]]],\n",
      "       dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)\n",
      "initial querys shape: torch.Size([1, 3, 1024])\n",
      "tensor([[[[-0.6250,  0.0942,  1.2578,  ..., -0.8867, -0.2471,  0.7070],\n",
      "          [ 0.3711,  0.2207,  0.3398,  ...,  0.0272, -0.6758, -0.0131],\n",
      "          [-0.1504,  0.4531, -0.6523,  ...,  0.7617, -1.1094,  0.5234],\n",
      "          [-0.4336, -0.1484,  0.0342,  ..., -0.6953,  0.4863, -0.1602]],\n",
      "\n",
      "         [[-0.0052,  0.0801, -0.5039,  ...,  0.3613, -0.0396,  0.3047],\n",
      "          [-0.2598,  0.8672, -0.6523,  ...,  0.7695, -1.1719, -0.0728],\n",
      "          [-0.3008, -0.3867, -0.4219,  ...,  0.4902, -0.2139,  0.5117],\n",
      "          [-0.5547,  0.6406, -0.2344,  ..., -1.1875,  0.0981, -0.7344]],\n",
      "\n",
      "         [[-0.1113,  0.2891, -0.6367,  ...,  0.2168, -0.7031,  0.0203],\n",
      "          [ 0.8477, -0.4766, -0.2217,  ..., -0.0020, -0.0474, -0.0972],\n",
      "          [-0.0806,  1.0234,  0.2148,  ...,  0.5742, -0.1855, -0.0728],\n",
      "          [-0.0087, -0.4238, -0.3711,  ...,  0.2383, -0.8008, -0.3125]]]],\n",
      "       dtype=torch.bfloat16, grad_fn=<ViewBackward0>)\n",
      "after view querys shape: torch.Size([1, 3, 4, 256])\n",
      "tensor([[[[-0.6250,  0.0942,  1.2578,  ..., -0.8867, -0.2471,  0.7070],\n",
      "          [-0.0052,  0.0801, -0.5039,  ...,  0.3613, -0.0396,  0.3047],\n",
      "          [-0.1113,  0.2891, -0.6367,  ...,  0.2168, -0.7031,  0.0203]],\n",
      "\n",
      "         [[ 0.3711,  0.2207,  0.3398,  ...,  0.0272, -0.6758, -0.0131],\n",
      "          [-0.2598,  0.8672, -0.6523,  ...,  0.7695, -1.1719, -0.0728],\n",
      "          [ 0.8477, -0.4766, -0.2217,  ..., -0.0020, -0.0474, -0.0972]],\n",
      "\n",
      "         [[-0.1504,  0.4531, -0.6523,  ...,  0.7617, -1.1094,  0.5234],\n",
      "          [-0.3008, -0.3867, -0.4219,  ...,  0.4902, -0.2139,  0.5117],\n",
      "          [-0.0806,  1.0234,  0.2148,  ...,  0.5742, -0.1855, -0.0728]],\n",
      "\n",
      "         [[-0.4336, -0.1484,  0.0342,  ..., -0.6953,  0.4863, -0.1602],\n",
      "          [-0.5547,  0.6406, -0.2344,  ..., -1.1875,  0.0981, -0.7344],\n",
      "          [-0.0087, -0.4238, -0.3711,  ...,  0.2383, -0.8008, -0.3125]]]],\n",
      "       dtype=torch.bfloat16, grad_fn=<TransposeBackward0>)\n",
      "after transpose querys shape: torch.Size([1, 4, 3, 256])\n",
      "tensor([[[-0.2656,  0.4336, -0.9219,  ..., -0.3027, -0.7578,  0.9688],\n",
      "         [-0.0664, -0.0874,  0.4004,  ..., -0.1641, -0.0957,  0.3789],\n",
      "         [-0.0718, -0.5742,  1.0703,  ...,  0.3848, -0.0500,  0.0361]]],\n",
      "       dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)\n",
      "initial querys shape: torch.Size([1, 3, 1024])\n",
      "tensor([[[[-0.2656,  0.4336, -0.9219,  ..., -0.6094,  0.4648, -0.7305],\n",
      "          [-0.5039,  0.1709,  0.6328,  ..., -0.5898, -0.5430,  0.7344],\n",
      "          [-0.9375, -0.0098, -0.6992,  ...,  0.1699, -0.3652, -0.2354],\n",
      "          [-0.0092,  0.6562,  0.2256,  ..., -0.3027, -0.7578,  0.9688]],\n",
      "\n",
      "         [[-0.0664, -0.0874,  0.4004,  ...,  0.1162,  0.1157,  0.1367],\n",
      "          [-0.4004,  0.2354,  0.8945,  ..., -0.6719,  0.0378, -1.3750],\n",
      "          [-0.3555,  0.1895, -0.1816,  ...,  0.1064,  0.2383, -0.4961],\n",
      "          [ 0.4824,  0.3789,  0.7852,  ..., -0.1641, -0.0957,  0.3789]],\n",
      "\n",
      "         [[-0.0718, -0.5742,  1.0703,  ..., -0.6289, -0.8438,  0.6523],\n",
      "          [-0.5391,  0.3125, -0.0050,  ..., -0.4980,  0.2334, -0.1895],\n",
      "          [-0.4805,  0.1631,  0.9453,  ..., -0.0186, -0.2383,  0.9844],\n",
      "          [-0.5742,  0.0574,  0.1260,  ...,  0.3848, -0.0500,  0.0361]]]],\n",
      "       dtype=torch.bfloat16, grad_fn=<ViewBackward0>)\n",
      "after view querys shape: torch.Size([1, 3, 4, 256])\n",
      "tensor([[[[-0.2656,  0.4336, -0.9219,  ..., -0.6094,  0.4648, -0.7305],\n",
      "          [-0.0664, -0.0874,  0.4004,  ...,  0.1162,  0.1157,  0.1367],\n",
      "          [-0.0718, -0.5742,  1.0703,  ..., -0.6289, -0.8438,  0.6523]],\n",
      "\n",
      "         [[-0.5039,  0.1709,  0.6328,  ..., -0.5898, -0.5430,  0.7344],\n",
      "          [-0.4004,  0.2354,  0.8945,  ..., -0.6719,  0.0378, -1.3750],\n",
      "          [-0.5391,  0.3125, -0.0050,  ..., -0.4980,  0.2334, -0.1895]],\n",
      "\n",
      "         [[-0.9375, -0.0098, -0.6992,  ...,  0.1699, -0.3652, -0.2354],\n",
      "          [-0.3555,  0.1895, -0.1816,  ...,  0.1064,  0.2383, -0.4961],\n",
      "          [-0.4805,  0.1631,  0.9453,  ..., -0.0186, -0.2383,  0.9844]],\n",
      "\n",
      "         [[-0.0092,  0.6562,  0.2256,  ..., -0.3027, -0.7578,  0.9688],\n",
      "          [ 0.4824,  0.3789,  0.7852,  ..., -0.1641, -0.0957,  0.3789],\n",
      "          [-0.5742,  0.0574,  0.1260,  ...,  0.3848, -0.0500,  0.0361]]]],\n",
      "       dtype=torch.bfloat16, grad_fn=<TransposeBackward0>)\n",
      "after transpose querys shape: torch.Size([1, 4, 3, 256])\n",
      "tensor([[[-0.3652,  0.9219, -0.8359,  ..., -0.2812,  0.0898,  0.8516],\n",
      "         [-0.1221,  0.7266,  0.6211,  ...,  0.7969,  0.2910,  0.3438],\n",
      "         [ 0.2158,  0.9023, -0.5352,  ...,  1.6406, -0.0669, -0.2930]]],\n",
      "       dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)\n",
      "initial querys shape: torch.Size([1, 3, 1024])\n",
      "tensor([[[[-0.3652,  0.9219, -0.8359,  ..., -0.3262, -0.1074, -0.0522],\n",
      "          [-0.0286, -0.2969,  0.0854,  ...,  0.8125, -0.8633, -0.5547],\n",
      "          [-0.5508,  0.1123, -0.5273,  ...,  0.1885, -0.3105,  0.7617],\n",
      "          [-0.0579,  0.4941,  0.8320,  ..., -0.2812,  0.0898,  0.8516]],\n",
      "\n",
      "         [[-0.1221,  0.7266,  0.6211,  ...,  0.0239, -0.1973, -0.0052],\n",
      "          [-0.8672, -0.0635,  0.9883,  ...,  0.8164,  0.2480,  0.3516],\n",
      "          [ 0.9688, -1.3906, -1.0781,  ...,  0.2773, -0.3945, -0.0781],\n",
      "          [ 0.0762, -0.9141, -0.1875,  ...,  0.7969,  0.2910,  0.3438]],\n",
      "\n",
      "         [[ 0.2158,  0.9023, -0.5352,  ..., -0.0806, -0.8711,  0.6719],\n",
      "          [-0.8945,  0.3965, -0.3301,  ..., -0.3828,  0.2930,  1.0234],\n",
      "          [-0.5898, -0.7812,  0.1426,  ..., -0.5703, -0.8008,  0.3438],\n",
      "          [ 0.5000,  1.1562,  0.3535,  ...,  1.6406, -0.0669, -0.2930]]]],\n",
      "       dtype=torch.bfloat16, grad_fn=<ViewBackward0>)\n",
      "after view querys shape: torch.Size([1, 3, 4, 256])\n",
      "tensor([[[[-0.3652,  0.9219, -0.8359,  ..., -0.3262, -0.1074, -0.0522],\n",
      "          [-0.1221,  0.7266,  0.6211,  ...,  0.0239, -0.1973, -0.0052],\n",
      "          [ 0.2158,  0.9023, -0.5352,  ..., -0.0806, -0.8711,  0.6719]],\n",
      "\n",
      "         [[-0.0286, -0.2969,  0.0854,  ...,  0.8125, -0.8633, -0.5547],\n",
      "          [-0.8672, -0.0635,  0.9883,  ...,  0.8164,  0.2480,  0.3516],\n",
      "          [-0.8945,  0.3965, -0.3301,  ..., -0.3828,  0.2930,  1.0234]],\n",
      "\n",
      "         [[-0.5508,  0.1123, -0.5273,  ...,  0.1885, -0.3105,  0.7617],\n",
      "          [ 0.9688, -1.3906, -1.0781,  ...,  0.2773, -0.3945, -0.0781],\n",
      "          [-0.5898, -0.7812,  0.1426,  ..., -0.5703, -0.8008,  0.3438]],\n",
      "\n",
      "         [[-0.0579,  0.4941,  0.8320,  ..., -0.2812,  0.0898,  0.8516],\n",
      "          [ 0.0762, -0.9141, -0.1875,  ...,  0.7969,  0.2910,  0.3438],\n",
      "          [ 0.5000,  1.1562,  0.3535,  ...,  1.6406, -0.0669, -0.2930]]]],\n",
      "       dtype=torch.bfloat16, grad_fn=<TransposeBackward0>)\n",
      "after transpose querys shape: torch.Size([1, 4, 3, 256])\n",
      "tensor([[[ 0.2422,  1.1953,  0.1895,  ...,  0.5547, -0.3711,  0.8125],\n",
      "         [ 0.4590,  0.2031,  0.0091,  ..., -0.4121,  0.3164,  0.3242],\n",
      "         [-1.0078,  0.4727, -0.3789,  ..., -0.1465, -0.5039,  0.8203]]],\n",
      "       dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)\n",
      "initial querys shape: torch.Size([1, 3, 1024])\n",
      "tensor([[[[ 0.2422,  1.1953,  0.1895,  ..., -0.8164, -0.4414, -0.7773],\n",
      "          [-0.7617, -0.1621,  0.3477,  ..., -0.0422,  1.3672,  0.4434],\n",
      "          [ 0.5703, -0.0947,  0.0579,  ...,  1.1016,  0.2695, -0.1445],\n",
      "          [ 1.2734, -0.1523, -0.0645,  ...,  0.5547, -0.3711,  0.8125]],\n",
      "\n",
      "         [[ 0.4590,  0.2031,  0.0091,  ...,  1.4688, -0.7305, -0.6953],\n",
      "          [-0.4785, -0.1602,  0.2598,  ...,  0.0898,  0.9414,  0.9375],\n",
      "          [ 0.5859,  0.1089, -0.0062,  ...,  0.6445,  0.6992, -0.0947],\n",
      "          [-1.6016,  0.8320,  0.9609,  ..., -0.4121,  0.3164,  0.3242]],\n",
      "\n",
      "         [[-1.0078,  0.4727, -0.3789,  ...,  0.2617,  0.6133, -0.6953],\n",
      "          [-0.1475, -0.8242, -0.5547,  ..., -0.3945,  0.1040,  0.8594],\n",
      "          [ 0.4492, -0.4512,  0.1611,  ..., -0.9180, -0.5078,  0.0391],\n",
      "          [-0.9219, -0.0386,  0.0703,  ..., -0.1465, -0.5039,  0.8203]]]],\n",
      "       dtype=torch.bfloat16, grad_fn=<ViewBackward0>)\n",
      "after view querys shape: torch.Size([1, 3, 4, 256])\n",
      "tensor([[[[ 0.2422,  1.1953,  0.1895,  ..., -0.8164, -0.4414, -0.7773],\n",
      "          [ 0.4590,  0.2031,  0.0091,  ...,  1.4688, -0.7305, -0.6953],\n",
      "          [-1.0078,  0.4727, -0.3789,  ...,  0.2617,  0.6133, -0.6953]],\n",
      "\n",
      "         [[-0.7617, -0.1621,  0.3477,  ..., -0.0422,  1.3672,  0.4434],\n",
      "          [-0.4785, -0.1602,  0.2598,  ...,  0.0898,  0.9414,  0.9375],\n",
      "          [-0.1475, -0.8242, -0.5547,  ..., -0.3945,  0.1040,  0.8594]],\n",
      "\n",
      "         [[ 0.5703, -0.0947,  0.0579,  ...,  1.1016,  0.2695, -0.1445],\n",
      "          [ 0.5859,  0.1089, -0.0062,  ...,  0.6445,  0.6992, -0.0947],\n",
      "          [ 0.4492, -0.4512,  0.1611,  ..., -0.9180, -0.5078,  0.0391]],\n",
      "\n",
      "         [[ 1.2734, -0.1523, -0.0645,  ...,  0.5547, -0.3711,  0.8125],\n",
      "          [-1.6016,  0.8320,  0.9609,  ..., -0.4121,  0.3164,  0.3242],\n",
      "          [-0.9219, -0.0386,  0.0703,  ..., -0.1465, -0.5039,  0.8203]]]],\n",
      "       dtype=torch.bfloat16, grad_fn=<TransposeBackward0>)\n",
      "after transpose querys shape: torch.Size([1, 4, 3, 256])\n",
      "tensor([[[ 1.1016,  0.0100,  0.1396,  ...,  0.1807, -0.1680, -0.7031],\n",
      "         [ 0.3828, -0.1553,  0.0894,  ...,  0.4023, -0.1533, -0.4766],\n",
      "         [ 1.1406,  0.0055,  0.0449,  ...,  0.2461, -0.3770, -0.4336]]],\n",
      "       dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)\n",
      "initial querys shape: torch.Size([1, 3, 1024])\n",
      "tensor([[[[ 1.1016e+00,  1.0010e-02,  1.3965e-01,  ..., -4.3945e-01,\n",
      "            1.6016e-01, -4.8047e-01],\n",
      "          [ 2.8125e-01, -1.1719e+00,  1.7773e-01,  ..., -2.7734e-01,\n",
      "            7.5391e-01, -5.0781e-01],\n",
      "          [-1.0010e-01, -3.3203e-02,  3.2227e-01,  ..., -5.2734e-01,\n",
      "            3.6133e-01,  9.8047e-01],\n",
      "          [ 5.6152e-02, -4.3945e-01,  5.0781e-01,  ...,  1.8066e-01,\n",
      "           -1.6797e-01, -7.0312e-01]],\n",
      "\n",
      "         [[ 3.8281e-01, -1.5527e-01,  8.9355e-02,  ...,  1.1719e+00,\n",
      "            4.2773e-01,  8.1250e-01],\n",
      "          [-4.0430e-01, -7.6172e-02,  5.5078e-01,  ...,  1.1172e+00,\n",
      "           -4.2969e-02, -1.0234e+00],\n",
      "          [ 5.9509e-04, -1.0205e-01, -2.6953e-01,  ...,  9.0332e-02,\n",
      "           -8.5547e-01, -6.2109e-01],\n",
      "          [ 7.9688e-01,  7.1094e-01,  1.3750e+00,  ...,  4.0234e-01,\n",
      "           -1.5332e-01, -4.7656e-01]],\n",
      "\n",
      "         [[ 1.1406e+00,  5.4932e-03,  4.4922e-02,  ..., -3.3398e-01,\n",
      "            1.8750e-01,  4.1797e-01],\n",
      "          [ 4.8633e-01,  4.7656e-01,  6.2500e-01,  ..., -2.6562e-01,\n",
      "           -1.1484e+00, -4.4531e-01],\n",
      "          [ 4.5312e-01, -2.3535e-01, -1.6504e-01,  ..., -1.8066e-01,\n",
      "            5.7422e-01,  4.2773e-01],\n",
      "          [ 1.1047e-02, -1.1572e-01, -2.0898e-01,  ...,  2.4609e-01,\n",
      "           -3.7695e-01, -4.3359e-01]]]], dtype=torch.bfloat16,\n",
      "       grad_fn=<ViewBackward0>)\n",
      "after view querys shape: torch.Size([1, 3, 4, 256])\n",
      "tensor([[[[ 1.1016e+00,  1.0010e-02,  1.3965e-01,  ..., -4.3945e-01,\n",
      "            1.6016e-01, -4.8047e-01],\n",
      "          [ 3.8281e-01, -1.5527e-01,  8.9355e-02,  ...,  1.1719e+00,\n",
      "            4.2773e-01,  8.1250e-01],\n",
      "          [ 1.1406e+00,  5.4932e-03,  4.4922e-02,  ..., -3.3398e-01,\n",
      "            1.8750e-01,  4.1797e-01]],\n",
      "\n",
      "         [[ 2.8125e-01, -1.1719e+00,  1.7773e-01,  ..., -2.7734e-01,\n",
      "            7.5391e-01, -5.0781e-01],\n",
      "          [-4.0430e-01, -7.6172e-02,  5.5078e-01,  ...,  1.1172e+00,\n",
      "           -4.2969e-02, -1.0234e+00],\n",
      "          [ 4.8633e-01,  4.7656e-01,  6.2500e-01,  ..., -2.6562e-01,\n",
      "           -1.1484e+00, -4.4531e-01]],\n",
      "\n",
      "         [[-1.0010e-01, -3.3203e-02,  3.2227e-01,  ..., -5.2734e-01,\n",
      "            3.6133e-01,  9.8047e-01],\n",
      "          [ 5.9509e-04, -1.0205e-01, -2.6953e-01,  ...,  9.0332e-02,\n",
      "           -8.5547e-01, -6.2109e-01],\n",
      "          [ 4.5312e-01, -2.3535e-01, -1.6504e-01,  ..., -1.8066e-01,\n",
      "            5.7422e-01,  4.2773e-01]],\n",
      "\n",
      "         [[ 5.6152e-02, -4.3945e-01,  5.0781e-01,  ...,  1.8066e-01,\n",
      "           -1.6797e-01, -7.0312e-01],\n",
      "          [ 7.9688e-01,  7.1094e-01,  1.3750e+00,  ...,  4.0234e-01,\n",
      "           -1.5332e-01, -4.7656e-01],\n",
      "          [ 1.1047e-02, -1.1572e-01, -2.0898e-01,  ...,  2.4609e-01,\n",
      "           -3.7695e-01, -4.3359e-01]]]], dtype=torch.bfloat16,\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "after transpose querys shape: torch.Size([1, 4, 3, 256])\n",
      "tensor([[[ 0.3398,  0.7500, -0.1079,  ..., -0.4824,  0.3496,  0.4785],\n",
      "         [ 0.4512,  0.1641, -0.5664,  ..., -0.5078,  0.0339,  0.6914],\n",
      "         [-0.0425,  0.0986, -0.8242,  ..., -0.6484,  0.1719,  0.3438]]],\n",
      "       dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)\n",
      "initial querys shape: torch.Size([1, 3, 1024])\n",
      "tensor([[[[ 0.3398,  0.7500, -0.1079,  ..., -0.2676,  0.5625,  1.8047],\n",
      "          [ 0.2373, -1.1797,  1.4062,  ...,  0.8203, -0.3359,  0.8398],\n",
      "          [ 0.6055, -0.3984, -0.8164,  ...,  0.0396,  0.0991,  0.2031],\n",
      "          [ 0.3105, -1.1328, -0.2031,  ..., -0.4824,  0.3496,  0.4785]],\n",
      "\n",
      "         [[ 0.4512,  0.1641, -0.5664,  ...,  1.0391, -0.1514, -0.1021],\n",
      "          [-0.2910, -0.5391,  0.7383,  ...,  0.6719,  0.3184,  0.8516],\n",
      "          [ 0.5586, -0.2490, -0.6797,  ..., -0.0099, -0.0630,  1.4141],\n",
      "          [-0.8477, -0.8867,  0.3613,  ..., -0.5078,  0.0339,  0.6914]],\n",
      "\n",
      "         [[-0.0425,  0.0986, -0.8242,  ...,  0.6289, -0.5742,  0.4180],\n",
      "          [-0.1475, -0.0542,  0.8477,  ...,  0.8047, -0.2910, -0.0298],\n",
      "          [-0.3848,  0.5312,  0.4648,  ..., -0.6914, -0.2051, -0.7930],\n",
      "          [-0.7695, -0.0197, -0.2578,  ..., -0.6484,  0.1719,  0.3438]]]],\n",
      "       dtype=torch.bfloat16, grad_fn=<ViewBackward0>)\n",
      "after view querys shape: torch.Size([1, 3, 4, 256])\n",
      "tensor([[[[ 0.3398,  0.7500, -0.1079,  ..., -0.2676,  0.5625,  1.8047],\n",
      "          [ 0.4512,  0.1641, -0.5664,  ...,  1.0391, -0.1514, -0.1021],\n",
      "          [-0.0425,  0.0986, -0.8242,  ...,  0.6289, -0.5742,  0.4180]],\n",
      "\n",
      "         [[ 0.2373, -1.1797,  1.4062,  ...,  0.8203, -0.3359,  0.8398],\n",
      "          [-0.2910, -0.5391,  0.7383,  ...,  0.6719,  0.3184,  0.8516],\n",
      "          [-0.1475, -0.0542,  0.8477,  ...,  0.8047, -0.2910, -0.0298]],\n",
      "\n",
      "         [[ 0.6055, -0.3984, -0.8164,  ...,  0.0396,  0.0991,  0.2031],\n",
      "          [ 0.5586, -0.2490, -0.6797,  ..., -0.0099, -0.0630,  1.4141],\n",
      "          [-0.3848,  0.5312,  0.4648,  ..., -0.6914, -0.2051, -0.7930]],\n",
      "\n",
      "         [[ 0.3105, -1.1328, -0.2031,  ..., -0.4824,  0.3496,  0.4785],\n",
      "          [-0.8477, -0.8867,  0.3613,  ..., -0.5078,  0.0339,  0.6914],\n",
      "          [-0.7695, -0.0197, -0.2578,  ..., -0.6484,  0.1719,  0.3438]]]],\n",
      "       dtype=torch.bfloat16, grad_fn=<TransposeBackward0>)\n",
      "after transpose querys shape: torch.Size([1, 4, 3, 256])\n",
      "tensor([[[-1.9766,  0.7383, -0.3672,  ..., -0.4805,  0.2285, -0.2520],\n",
      "         [-0.2178, -1.0391,  0.2451,  ..., -0.8398, -0.2598,  0.0559],\n",
      "         [ 0.7812, -1.3203,  0.3887,  ...,  0.1914, -0.2578, -0.9570]]],\n",
      "       dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)\n",
      "initial querys shape: torch.Size([1, 3, 1024])\n",
      "tensor([[[[-1.9766,  0.7383, -0.3672,  ...,  0.2910, -0.4199,  0.1250],\n",
      "          [ 0.5508, -0.1309, -0.3691,  ...,  0.4805,  0.8242,  0.4082],\n",
      "          [ 0.4219, -0.0918,  0.0378,  ...,  0.6289, -0.0374, -1.0234],\n",
      "          [-0.9883,  0.1436,  0.3828,  ..., -0.4805,  0.2285, -0.2520]],\n",
      "\n",
      "         [[-0.2178, -1.0391,  0.2451,  ..., -0.9648, -0.4375, -0.9531],\n",
      "          [ 0.1748, -0.5391,  0.1040,  ...,  0.7383,  0.6719, -0.3555],\n",
      "          [-0.5781, -0.4473, -0.4316,  ..., -0.5078,  0.3086, -0.0427],\n",
      "          [-0.7031, -0.4922,  0.1689,  ..., -0.8398, -0.2598,  0.0559]],\n",
      "\n",
      "         [[ 0.7812, -1.3203,  0.3887,  ...,  0.1973, -0.0128, -0.5859],\n",
      "          [ 0.3633,  0.0581,  0.1758,  ..., -0.3223, -0.1089,  0.5039],\n",
      "          [-0.6641, -0.9766,  0.3105,  ..., -0.1904,  0.2598,  0.1001],\n",
      "          [ 0.4375,  0.1562, -0.5273,  ...,  0.1914, -0.2578, -0.9570]]]],\n",
      "       dtype=torch.bfloat16, grad_fn=<ViewBackward0>)\n",
      "after view querys shape: torch.Size([1, 3, 4, 256])\n",
      "tensor([[[[-1.9766,  0.7383, -0.3672,  ...,  0.2910, -0.4199,  0.1250],\n",
      "          [-0.2178, -1.0391,  0.2451,  ..., -0.9648, -0.4375, -0.9531],\n",
      "          [ 0.7812, -1.3203,  0.3887,  ...,  0.1973, -0.0128, -0.5859]],\n",
      "\n",
      "         [[ 0.5508, -0.1309, -0.3691,  ...,  0.4805,  0.8242,  0.4082],\n",
      "          [ 0.1748, -0.5391,  0.1040,  ...,  0.7383,  0.6719, -0.3555],\n",
      "          [ 0.3633,  0.0581,  0.1758,  ..., -0.3223, -0.1089,  0.5039]],\n",
      "\n",
      "         [[ 0.4219, -0.0918,  0.0378,  ...,  0.6289, -0.0374, -1.0234],\n",
      "          [-0.5781, -0.4473, -0.4316,  ..., -0.5078,  0.3086, -0.0427],\n",
      "          [-0.6641, -0.9766,  0.3105,  ..., -0.1904,  0.2598,  0.1001]],\n",
      "\n",
      "         [[-0.9883,  0.1436,  0.3828,  ..., -0.4805,  0.2285, -0.2520],\n",
      "          [-0.7031, -0.4922,  0.1689,  ..., -0.8398, -0.2598,  0.0559],\n",
      "          [ 0.4375,  0.1562, -0.5273,  ...,  0.1914, -0.2578, -0.9570]]]],\n",
      "       dtype=torch.bfloat16, grad_fn=<TransposeBackward0>)\n",
      "after transpose querys shape: torch.Size([1, 4, 3, 256])\n",
      "tensor([[[ 0.0757, -0.1523,  1.0703,  ...,  0.5430, -0.2891,  1.2734],\n",
      "         [ 0.3203, -0.7969,  0.2324,  ...,  0.4180,  0.1855,  0.4258],\n",
      "         [-1.4297, -0.4785,  0.4199,  ...,  0.3965,  1.0547,  0.8008]]],\n",
      "       dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)\n",
      "initial querys shape: torch.Size([1, 3, 1024])\n",
      "tensor([[[[ 0.0757, -0.1523,  1.0703,  ..., -0.6445,  0.0820, -0.0073],\n",
      "          [ 0.8438,  0.7305, -0.9805,  ...,  0.4297, -0.2471,  0.5273],\n",
      "          [ 0.6016, -0.2256,  0.1611,  ..., -0.2812, -0.1445, -0.0038],\n",
      "          [ 0.2012, -0.0659, -0.6016,  ...,  0.5430, -0.2891,  1.2734]],\n",
      "\n",
      "         [[ 0.3203, -0.7969,  0.2324,  ...,  0.9414,  0.0040, -0.4180],\n",
      "          [ 0.2891,  0.6562, -1.1953,  ...,  0.7344, -0.7227, -0.2852],\n",
      "          [-0.1689, -0.8984,  0.9883,  ..., -0.9297, -0.0245,  0.0413],\n",
      "          [ 0.5117,  0.7422,  0.1445,  ...,  0.4180,  0.1855,  0.4258]],\n",
      "\n",
      "         [[-1.4297, -0.4785,  0.4199,  ...,  0.2158, -0.0188,  0.8164],\n",
      "          [-0.1152,  0.3516,  0.3008,  ...,  0.2383,  0.2637, -0.6484],\n",
      "          [ 0.0113,  0.9141, -0.1729,  ...,  0.2129, -0.0243, -0.2871],\n",
      "          [-0.7539, -0.2461,  1.3594,  ...,  0.3965,  1.0547,  0.8008]]]],\n",
      "       dtype=torch.bfloat16, grad_fn=<ViewBackward0>)\n",
      "after view querys shape: torch.Size([1, 3, 4, 256])\n",
      "tensor([[[[ 0.0757, -0.1523,  1.0703,  ..., -0.6445,  0.0820, -0.0073],\n",
      "          [ 0.3203, -0.7969,  0.2324,  ...,  0.9414,  0.0040, -0.4180],\n",
      "          [-1.4297, -0.4785,  0.4199,  ...,  0.2158, -0.0188,  0.8164]],\n",
      "\n",
      "         [[ 0.8438,  0.7305, -0.9805,  ...,  0.4297, -0.2471,  0.5273],\n",
      "          [ 0.2891,  0.6562, -1.1953,  ...,  0.7344, -0.7227, -0.2852],\n",
      "          [-0.1152,  0.3516,  0.3008,  ...,  0.2383,  0.2637, -0.6484]],\n",
      "\n",
      "         [[ 0.6016, -0.2256,  0.1611,  ..., -0.2812, -0.1445, -0.0038],\n",
      "          [-0.1689, -0.8984,  0.9883,  ..., -0.9297, -0.0245,  0.0413],\n",
      "          [ 0.0113,  0.9141, -0.1729,  ...,  0.2129, -0.0243, -0.2871]],\n",
      "\n",
      "         [[ 0.2012, -0.0659, -0.6016,  ...,  0.5430, -0.2891,  1.2734],\n",
      "          [ 0.5117,  0.7422,  0.1445,  ...,  0.4180,  0.1855,  0.4258],\n",
      "          [-0.7539, -0.2461,  1.3594,  ...,  0.3965,  1.0547,  0.8008]]]],\n",
      "       dtype=torch.bfloat16, grad_fn=<TransposeBackward0>)\n",
      "after transpose querys shape: torch.Size([1, 4, 3, 256])\n",
      "tensor([[[ 0.6680,  0.6055,  0.6016,  ..., -0.2227, -0.3418,  0.1797],\n",
      "         [-0.3164,  0.4277,  0.2949,  ...,  0.0593, -0.0210, -0.1768],\n",
      "         [-0.0640,  0.1553, -0.6680,  ...,  0.0967, -0.5781, -0.6562]]],\n",
      "       dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)\n",
      "initial querys shape: torch.Size([1, 3, 1024])\n",
      "tensor([[[[ 0.6680,  0.6055,  0.6016,  ..., -0.0405, -0.1934, -0.2598],\n",
      "          [ 0.2197,  0.2695,  0.1816,  ..., -0.0908,  0.0018,  0.0437],\n",
      "          [-0.4941,  0.2051, -0.4023,  ..., -0.6250, -1.2031, -0.0077],\n",
      "          [-0.2812, -0.1475, -0.9727,  ..., -0.2227, -0.3418,  0.1797]],\n",
      "\n",
      "         [[-0.3164,  0.4277,  0.2949,  ...,  0.2227,  0.3750,  0.0087],\n",
      "          [ 0.4102,  0.5820,  0.8438,  ...,  0.6367, -0.0288,  0.1416],\n",
      "          [ 0.7500, -1.1406, -1.4922,  ..., -0.5625, -0.4688,  0.1128],\n",
      "          [ 1.0781, -0.0054, -0.7617,  ...,  0.0593, -0.0210, -0.1768]],\n",
      "\n",
      "         [[-0.0640,  0.1553, -0.6680,  ..., -0.7188,  0.1758,  0.1289],\n",
      "          [ 1.0547, -0.3691,  0.2500,  ..., -0.3477, -0.4551,  0.1611],\n",
      "          [-0.2266,  0.4180,  0.4453,  ...,  0.0099,  0.0898, -0.6367],\n",
      "          [ 0.6055, -0.1602, -0.4902,  ...,  0.0967, -0.5781, -0.6562]]]],\n",
      "       dtype=torch.bfloat16, grad_fn=<ViewBackward0>)\n",
      "after view querys shape: torch.Size([1, 3, 4, 256])\n",
      "tensor([[[[ 0.6680,  0.6055,  0.6016,  ..., -0.0405, -0.1934, -0.2598],\n",
      "          [-0.3164,  0.4277,  0.2949,  ...,  0.2227,  0.3750,  0.0087],\n",
      "          [-0.0640,  0.1553, -0.6680,  ..., -0.7188,  0.1758,  0.1289]],\n",
      "\n",
      "         [[ 0.2197,  0.2695,  0.1816,  ..., -0.0908,  0.0018,  0.0437],\n",
      "          [ 0.4102,  0.5820,  0.8438,  ...,  0.6367, -0.0288,  0.1416],\n",
      "          [ 1.0547, -0.3691,  0.2500,  ..., -0.3477, -0.4551,  0.1611]],\n",
      "\n",
      "         [[-0.4941,  0.2051, -0.4023,  ..., -0.6250, -1.2031, -0.0077],\n",
      "          [ 0.7500, -1.1406, -1.4922,  ..., -0.5625, -0.4688,  0.1128],\n",
      "          [-0.2266,  0.4180,  0.4453,  ...,  0.0099,  0.0898, -0.6367]],\n",
      "\n",
      "         [[-0.2812, -0.1475, -0.9727,  ..., -0.2227, -0.3418,  0.1797],\n",
      "          [ 1.0781, -0.0054, -0.7617,  ...,  0.0593, -0.0210, -0.1768],\n",
      "          [ 0.6055, -0.1602, -0.4902,  ...,  0.0967, -0.5781, -0.6562]]]],\n",
      "       dtype=torch.bfloat16, grad_fn=<TransposeBackward0>)\n",
      "after transpose querys shape: torch.Size([1, 4, 3, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7539,  0.1060,  0.4805,  ...,  0.9375,  0.4043, -0.2383],\n",
       "         [-0.3418, -0.0576,  0.8984,  ..., -0.2432,  0.4629,  0.8242],\n",
       "         [-0.2695, -0.3281,  0.4102,  ...,  0.8750, -0.9727,  0.9844]]],\n",
       "       dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor([1, 2, 3]).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "364e76ca-52f8-4fa5-af37-c4069f9694bc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "364e76ca-52f8-4fa5-af37-c4069f9694bc",
    "outputId": "00d7e983-262e-4c65-f322-f4d999311988"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 435,870,336\n",
      "\n",
      "Total number of unique parameters: 268,098,176\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")\n",
    "\n",
    "# Account for weight tying\n",
    "total_params_normalized = total_params - model.tok_emb.weight.numel()\n",
    "print(f\"\\nTotal number of unique parameters: {total_params_normalized:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd5efb03-5a07-46e8-8607-93ed47549d2b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fd5efb03-5a07-46e8-8607-93ed47549d2b",
    "outputId": "65c1a95e-b502-4150-9e2e-da619d9053d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32 (PyTorch default): 3.37 GB\n",
      "bfloat16: 1.69 GB\n"
     ]
    }
   ],
   "source": [
    "def model_memory_size(model, input_dtype=torch.float32):\n",
    "    total_params = 0\n",
    "    total_grads = 0\n",
    "    for param in model.parameters():\n",
    "        # Calculate total number of elements per parameter\n",
    "        param_size = param.numel()\n",
    "        total_params += param_size\n",
    "        # Check if gradients are stored for this parameter\n",
    "        if param.requires_grad:\n",
    "            total_grads += param_size\n",
    "\n",
    "    # Calculate buffer size (non-parameters that require memory)\n",
    "    total_buffers = sum(buf.numel() for buf in model.buffers())\n",
    "\n",
    "    # Size in bytes = (Number of elements) * (Size of each element in bytes)\n",
    "    # We assume parameters and gradients are stored in the same type as input dtype\n",
    "    element_size = torch.tensor(0, dtype=input_dtype).element_size()\n",
    "    total_memory_bytes = (total_params + total_grads + total_buffers) * element_size\n",
    "\n",
    "    # Convert bytes to gigabytes\n",
    "    total_memory_gb = total_memory_bytes / (1024**3)\n",
    "\n",
    "    return total_memory_gb\n",
    "\n",
    "print(f\"float32 (PyTorch default): {model_memory_size(model, input_dtype=torch.float32):.2f} GB\")\n",
    "print(f\"bfloat16: {model_memory_size(model, input_dtype=torch.bfloat16):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31f12baf-f79b-499f-85c0-51328a6a20f5",
   "metadata": {
    "id": "31f12baf-f79b-499f-85c0-51328a6a20f5"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c172f89f-d301-439f-b809-46169e5f5945",
   "metadata": {
    "id": "c172f89f-d301-439f-b809-46169e5f5945"
   },
   "source": [
    "&nbsp;\n",
    "# 4. Load pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75166128-5899-4995-9b88-9672e135650e",
   "metadata": {
    "id": "75166128-5899-4995-9b88-9672e135650e"
   },
   "outputs": [],
   "source": [
    "def load_weights_into_gemma(Gemma3Model, param_config, params):\n",
    "\n",
    "    def assign(left, right, tensor_name=\"unknown\"):\n",
    "        if left.shape != right.shape:\n",
    "            raise ValueError(\n",
    "                f\"Shape mismatch in tensor '{tensor_name}'. Left: {left.shape}, Right: {right.shape}\"\n",
    "            )\n",
    "        return torch.nn.Parameter(right.clone().detach() if isinstance(right, torch.Tensor) else torch.tensor(right))\n",
    "\n",
    "    # Embedding weights\n",
    "    if \"model.embed_tokens.weight\" in params:\n",
    "        model.tok_emb.weight = assign(\n",
    "            model.tok_emb.weight,\n",
    "            params[\"model.embed_tokens.weight\"],\n",
    "            \"model.embed_tokens.weight\",\n",
    "        )\n",
    "\n",
    "    # Iterate over transformer layers\n",
    "    for l in range(param_config[\"n_layers\"]):\n",
    "        block = model.blocks[l]\n",
    "        att = block.att\n",
    "        # Attention projections\n",
    "        att.W_query.weight = assign(\n",
    "            att.W_query.weight,\n",
    "            params[f\"model.layers.{l}.self_attn.q_proj.weight\"],\n",
    "            f\"model.layers.{l}.self_attn.q_proj.weight\",\n",
    "        )\n",
    "        att.W_key.weight = assign(\n",
    "            att.W_key.weight,\n",
    "            params[f\"model.layers.{l}.self_attn.k_proj.weight\"],\n",
    "            f\"model.layers.{l}.self_attn.k_proj.weight\",\n",
    "        )\n",
    "        att.W_value.weight = assign(\n",
    "            att.W_value.weight,\n",
    "            params[f\"model.layers.{l}.self_attn.v_proj.weight\"],\n",
    "            f\"model.layers.{l}.self_attn.v_proj.weight\",\n",
    "        )\n",
    "        att.out_proj.weight = assign(\n",
    "            att.out_proj.weight,\n",
    "            params[f\"model.layers.{l}.self_attn.o_proj.weight\"],\n",
    "            f\"model.layers.{l}.self_attn.o_proj.weight\",\n",
    "        )\n",
    "        # QK normalization weights\n",
    "        att.q_norm.scale = assign(\n",
    "            att.q_norm.scale,\n",
    "            params[f\"model.layers.{l}.self_attn.q_norm.weight\"],\n",
    "            f\"model.layers.{l}.self_attn.q_norm.weight\",\n",
    "        )\n",
    "        att.k_norm.scale = assign(\n",
    "            att.k_norm.scale,\n",
    "            params[f\"model.layers.{l}.self_attn.k_norm.weight\"],\n",
    "            f\"model.layers.{l}.self_attn.k_norm.weight\",\n",
    "        )\n",
    "        # Feed forward weights\n",
    "        block.ff.fc1.weight = assign(\n",
    "            block.ff.fc1.weight,\n",
    "            params[f\"model.layers.{l}.mlp.gate_proj.weight\"],\n",
    "            f\"model.layers.{l}.mlp.gate_proj.weight\",\n",
    "        )\n",
    "        block.ff.fc2.weight = assign(\n",
    "            block.ff.fc2.weight,\n",
    "            params[f\"model.layers.{l}.mlp.up_proj.weight\"],\n",
    "            f\"model.layers.{l}.mlp.up_proj.weight\",\n",
    "        )\n",
    "        block.ff.fc3.weight = assign(\n",
    "            block.ff.fc3.weight,\n",
    "            params[f\"model.layers.{l}.mlp.down_proj.weight\"],\n",
    "            f\"model.layers.{l}.mlp.down_proj.weight\",\n",
    "        )\n",
    "        # LayerNorm weights\n",
    "        block.input_layernorm.scale = assign(\n",
    "            block.input_layernorm.scale,\n",
    "            params[f\"model.layers.{l}.input_layernorm.weight\"],\n",
    "            f\"model.layers.{l}.input_layernorm.weight\",\n",
    "        )\n",
    "        block.post_attention_layernorm.scale = assign(\n",
    "            block.post_attention_layernorm.scale,\n",
    "            params[f\"model.layers.{l}.post_attention_layernorm.weight\"],\n",
    "            f\"model.layers.{l}.post_attention_layernorm.weight\",\n",
    "        )\n",
    "        # Pre‑ and post‑feed forward norms\n",
    "        pre_key = f\"model.layers.{l}.pre_feedforward_layernorm.weight\"\n",
    "        post_key = f\"model.layers.{l}.post_feedforward_layernorm.weight\"\n",
    "        if pre_key in params:\n",
    "            block.pre_feedforward_layernorm.scale = assign(\n",
    "                block.pre_feedforward_layernorm.scale,\n",
    "                params[pre_key],\n",
    "                pre_key,\n",
    "            )\n",
    "        if post_key in params:\n",
    "            block.post_feedforward_layernorm.scale = assign(\n",
    "                block.post_feedforward_layernorm.scale,\n",
    "                params[post_key],\n",
    "                post_key,\n",
    "            )\n",
    "\n",
    "    # Final LayerNorm\n",
    "    if \"model.norm.weight\" in params:\n",
    "        model.final_norm.scale = assign(\n",
    "            model.final_norm.scale,\n",
    "            params[\"model.norm.weight\"],\n",
    "            \"model.norm.weight\",\n",
    "        )\n",
    "    # Output head\n",
    "    if \"lm_head.weight\" in params:\n",
    "        model.out_head.weight = assign(\n",
    "            model.out_head.weight,\n",
    "            params[\"lm_head.weight\"],\n",
    "            \"lm_head.weight\",\n",
    "        )\n",
    "    elif \"model.embed_tokens.weight\" in params:\n",
    "        # Weight tying: reuse the embedding weights\n",
    "        model.out_head.weight = assign(\n",
    "            model.out_head.weight,\n",
    "            params[\"model.embed_tokens.weight\"],\n",
    "            \"model.embed_tokens.weight\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430340f2-78b9-4983-b74e-8395bbd7e574",
   "metadata": {},
   "source": [
    "- Please note that Google requires that you accept the Gemma 3 licensing terms before you can download the files; to do this, you have to create a Hugging Face Hub account and visit the [google/gemma-3-270m]https://huggingface.co/google/gemma-3-270m) repository to accept the terms\n",
    "- Next, you will need to create an access token; to generate an access token with READ permissions, click on the profile picture in the upper right and click on \"Settings\"\n",
    "\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/gpt-to-llama/settings.webp?1\" width=\"300px\">\n",
    "\n",
    "- Then, create and copy the access token so you can copy & paste it into the next code cell\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/gpt-to-llama/access-token.webp?1\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7cee5292-f756-41dd-9b8d-c9b5c25d23f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run the following code if you are executing the notebook for the first time\n",
    "\n",
    "#from huggingface_hub import login\n",
    "#login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "699cb1b8-a67d-49fb-80a6-0dad9d81f392",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "9881b6995c3f49dc89e6992fd9ab660b",
      "17a3174e65c54476b2e0d1faf8f011ca",
      "1bbf2e62c0754d1593beb4105a7f1ac1",
      "b82112e1dec645d98aa1c1ba64abcb61",
      "271e2bd6a35e4a8b92de8697f7c0be5f",
      "90a79523187446dfa692723b2e5833a7",
      "431ffb83b8c14bf182f0430e07ea6154",
      "a8f1b72a33dd4b548de23fbd95e0da18",
      "25cc36132d384189acfbecc59483134b",
      "bfd06423ad544218968648016e731a46",
      "d029630b63ff44cf807ade428d2eb421"
     ]
    },
    "id": "699cb1b8-a67d-49fb-80a6-0dad9d81f392",
    "outputId": "55b2f28c-142f-4698-9d23-d27456d3ed6d"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from safetensors.torch import load_file\n",
    "from huggingface_hub import hf_hub_download, snapshot_download\n",
    "\n",
    "CHOOSE_MODEL = \"270m\"\n",
    "\n",
    "if USE_INSTRUCT_MODEL:\n",
    "    repo_id = f\"google/gemma-3-{CHOOSE_MODEL}-it\"\n",
    "else:\n",
    "    repo_id = f\"google/gemma-3-{CHOOSE_MODEL}\"\n",
    "\n",
    "\n",
    "local_dir = Path(repo_id).parts[-1]\n",
    "\n",
    "if CHOOSE_MODEL == \"270m\":\n",
    "    weights_file = hf_hub_download(\n",
    "        repo_id=repo_id,\n",
    "        filename=\"model.safetensors\",\n",
    "        local_dir=local_dir,\n",
    "    )\n",
    "    weights_dict = load_file(weights_file)\n",
    "else:\n",
    "    repo_dir = snapshot_download(repo_id=repo_id, local_dir=local_dir)\n",
    "    index_path = os.path.join(repo_dir, \"model.safetensors.index.json\")\n",
    "    with open(index_path, \"r\") as f:\n",
    "        index = json.load(f)\n",
    "\n",
    "    weights_dict = {}\n",
    "    for filename in set(index[\"weight_map\"].values()):\n",
    "        shard_path = os.path.join(repo_dir, filename)\n",
    "        shard = load_file(shard_path)\n",
    "        weights_dict.update(shard)\n",
    "\n",
    "load_weights_into_gemma(model, GEMMA3_CONFIG_270M, weights_dict)\n",
    "model.to(device)\n",
    "del weights_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b345491-3510-4397-92d3-cd0a3fa3deee",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "# 4. Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b68ab489-48e5-471e-a814-56cda2d60f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "\n",
    "class GemmaTokenizer:\n",
    "    def __init__(self, tokenizer_file_path: str):\n",
    "        tok_file = Path(tokenizer_file_path)\n",
    "        self._tok = Tokenizer.from_file(str(tok_file))\n",
    "        # Attempt to identify EOS and padding tokens\n",
    "        eos_token = \"<end_of_turn>\"\n",
    "        self.pad_token_id = eos_token\n",
    "        self.eos_token_id = eos_token\n",
    "\n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        return self._tok.encode(text).ids\n",
    "\n",
    "    def decode(self, ids: list[int]) -> str:\n",
    "        return self._tok.decode(ids, skip_special_tokens=False)\n",
    "\n",
    "\n",
    "def apply_chat_template(user_text):\n",
    "    return f\"<start_of_turn>user\\n{user_text}<end_of_turn>\\n<start_of_turn>model\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b6df8bc-7308-468e-93ce-2d5529ea7866",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_file_path = os.path.join(local_dir, \"tokenizer.json\")\n",
    "if not os.path.exists(tokenizer_file_path):\n",
    "    try:\n",
    "        tokenizer_file_path = hf_hub_download(repo_id=repo_id, filename=\"tokenizer.json\", local_dir=local_dir)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: failed to download tokenizer.json: {e}\")\n",
    "        tokenizer_file_path = \"tokenizer.json\"\n",
    "\n",
    "tokenizer = GemmaTokenizer(tokenizer_file_path=tokenizer_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1946b534-e3af-431a-a222-391a60bfa892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos><start_of_turn>user\\nGive me a short introduction to large language models.<end_of_turn>\\n<start_of_turn>model\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Give me a short introduction to large language models.\"\n",
    "prompt = apply_chat_template(\"Give me a short introduction to large language models.\")\n",
    "\n",
    "\n",
    "input_token_ids = tokenizer.encode(prompt)\n",
    "text = tokenizer.decode(input_token_ids)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d07df1-4401-4792-b549-7c4cc5632323",
   "metadata": {
    "id": "57d07df1-4401-4792-b549-7c4cc5632323"
   },
   "source": [
    "&nbsp;\n",
    "# 5. Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8401c6-e244-4cb7-9849-2ba71ce758d5",
   "metadata": {
    "id": "7b8401c6-e244-4cb7-9849-2ba71ce758d5"
   },
   "outputs": [],
   "source": [
    "def generate_text_basic_stream(model, token_ids, max_new_tokens, eos_token_id=None):\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            out = model(token_ids)[:, -1]\n",
    "            next_token = torch.argmax(out, dim=-1, keepdim=True)\n",
    "\n",
    "            if (eos_token_id is not None\n",
    "                   and torch.all(next_token == eos_token_id)):\n",
    "               break\n",
    "\n",
    "            yield next_token\n",
    "\n",
    "            token_ids = torch.cat([token_ids, next_token], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1c7a04fa-6aac-416b-8f63-f1e19227633d",
   "metadata": {
    "id": "1c7a04fa-6aac-416b-8f63-f1e19227633d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large language models (LLMs) are sophisticated artificial intelligence systems that can understand, generate, and manipulate human language. They are trained on massive amounts of text data to learn patterns and relationships within that data, enabling them to perform a wide range of tasks, from writing articles and answering questions to translating languages and summarizing information.\n"
     ]
    }
   ],
   "source": [
    "input_token_ids_tensor = torch.tensor(input_token_ids, device=device).unsqueeze(0)\n",
    "\n",
    "\n",
    "for token in generate_text_basic_stream(\n",
    "    model=model,\n",
    "    token_ids=input_token_ids_tensor,\n",
    "    max_new_tokens=500,\n",
    "    eos_token_id=tokenizer.encode(\"<end_of_turn>\")[-1]\n",
    "):\n",
    "    token_id = token.squeeze(0).tolist()\n",
    "    print(\n",
    "        tokenizer.decode(token_id),\n",
    "        end=\"\",\n",
    "        flush=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549324d6-5c71-4147-ae21-2e67675faa3d",
   "metadata": {
    "id": "549324d6-5c71-4147-ae21-2e67675faa3d"
   },
   "source": [
    "&nbsp;\n",
    "# What's next?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6edaaae-2de1-406c-8ffa-897cdfa3808c",
   "metadata": {
    "id": "e6edaaae-2de1-406c-8ffa-897cdfa3808c"
   },
   "source": [
    "- Check out the [README.md](./README.md), to use this model via the `llms_from_scratch` package\n",
    "- For those interested in a comprehensive guide on building a large language model from scratch and gaining a deeper understanding of its mechanics, you might like my [Build a Large Language Model (From Scratch)](http://mng.bz/orYv)\n",
    "\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
